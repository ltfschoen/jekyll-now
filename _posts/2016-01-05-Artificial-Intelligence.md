---
layout: post
title: Artificial Intelligence Nanodegree (in progress!)
---

# Table of Contents
  * [Chapter 1 - Artificial Intelligence Nanodegree](#chapter-1)
  
## Chapter 1 - Artificial Intelligence<a id="chapter-1"></a>

### Background and Definition

* AI comprises design and building intelligent entities/agents and requires
    * Intelligence
    * Artifact (i.e. computer)
* AI methods concern:
    * Human Thinking Automation (i.e. decision-making, problem solving, learning)
        * Cognitive Science Modelling Approach
            * Build precise and testable theory to express as 
            computer program algorithm AI models
            with matching input/output behaviour including 
            trace of reasoning steps when solving problems 
            comparable to human performance (similar to General Problem Solver of 
            1961), using:
                * Introspection (i.e. of human thoughts)
                * Psychological Experiments (i.e. observe human action)
                * Brain Imaging (i.e. observe brain action, incorporating
                neurophysiological evidence into computer models)
    * Mental Computation Models (i.e. perception, reasoning, action)
    * Behaviour
* AI measures success against:
    * Human Intelligence Performance Comparison (i.e. Turing Test 
    to see if can decipher whether response by human or computer
    using Computer Vision  and Robotics for object perception/manipulation)
        * Computer Capabilities
            * Natural Language Processing (i.e. communicates in English)
            * Knowledge Representation (i.e. store current knowledge, store received/heard info)
            * Automated Reasoning (i.e. reason and respond to requests using stored data)
            * Machine Learning (i.e. adapt to new situations and detect/deal with patterns)
    * Rationality (i.e. ideal performance with given data and assumptions)
        * THOUGHT - Logic Notation (discovered by Aristotle) by reasoning with
        statements about any object and relationships between
        with guidance on reasoning steps to try first to minimise computational 
        resources to solve problems in "principle" using 
        Intelligent Computational Reasoning Systems 
        (differs from solving in "practice")
        * ACTION (by rational agent) - 
            * Autonomous operation
            * Perception of environment
            * Adapt to change
            * Uptime maintained
            * Goal pursuit
            * Deliver best outcome/decision 
            * Rationality using inference given uncertainy (using 
            Knowledge Representation)
* AI approach to:
    * Simple Probems
        * Hypothesis that perfect rationality required for analysis
    * Complex Problems
        * Realise that unable to achieve perfect rationality requiring
         high computational demands 
* Definitions (of Mind and Matter)
    * Rationalism - Reasoning to understand the world
    * Dualism - Human mind has dual qualities governed by 
    both soul/spiritual laws and physical laws
    * Materialism - Human mind is constituted from brain operation
    according to laws of physics
    * Empirism
        * Flow of Knowledge 
            * Source of Knowledge
            * Physical mind uses Senses to manipulate Knowledge with 
            reasoning to achieve Understanding
    * Logical Positivism (Doctrine) - Combines Rationalism and Empiricism
    whereby Knowledge arises from interconnected logical theories connected
    to Observation Sentences corresponding to Sensory Inputs
    * Free will - Perception of choices to a choosing entity
    * Induction Principle - Senses are reliant on rules (Confirmation Theory)
    when acquiring Knowledge from their Experience 
    (exposure to repeated associations between elements)
    * Actions (occur after Reasoning) - Actions justified through
    logical connection between Goals, and Knowledge of an Action's 
    outcome (assuming the possible end outcome based on Experience)
* Rational Decision Theory
    * Issues
        * Decision-making Rules when several Actions achieve same Goal
        * Decision-making Rules when no Action achieves Goal
        * Decision-making Rules when uncertainty
    * AI Science
        * Maths (areas contributing to AI)
            * Logic
                * Algorithms using maths reasoning for logical deduction.
                Procedure to prove any true statement in boolean
                first-order logic of objects and relations,
                but first-order logic does not capture principle of maths
                induction required to characterise natural numbers.
                Incompleteness Theorem proved that in arithmetic (elementary
                natural numbers) some true statements are undecidable 
                (no proof within the theory), whereby some functions on
                integers cannot be represented by an algoritm.
                * Turing Machine was developed and showed some functions
                could not be Computed
                * Theory of References related logic objects with 
                real-world objects
            * Computation
                * **Tractability** - (easy) problems where time required to solve instance of
                problem grows with Polynomial growth.
                Tractable sub-problems should be approached when generating
                intelligent behaviour 
                    * Polynomial growth
                * **Intractable** (hard) problems - time required to solve instance of 
                problem grows Exponentially with size of instances
                * **Exponential growth** in complexity means even moderately
                large problem instances cannot be solved in reasonable timeframe
                * **NP-completeness Theory** showed there are many
                NP-complete problems (combination of search and
                reasoning), such that if a given problem class may be reduced to 
                one of these NP-complete problems, it is likely Intractable
            * Probability
                * Quantitative Science 
                    * Bayes' Rule used for uncertain measurements/reasoning
                    of AI systems. Statistical methods used to overcome
                    incomplete theories
        * Science
            * Economics
                * Definitions:
                    * Economies comprise individual agents making choices
                    that lead to preferred outcomes that maximise
                    economic well-being
                    * Sellers may assert they prefer money whilst they hope 
                    Buyers prefer their goods
                * Large Economies
                    * Decision Theory 
                    (framework for decisions under uncertain environment)
                        * Probability Theory
                        * Utility Theory
                        * **Usage**: Suitable for large economies where individual 
                        agents should not be concerned about Actions undertaken 
                        jointly by multiple other agents
                * Small Economies
                    * Game Theory
                        * **Usage**: Suitable for small economies where individual agents
                        may be significantly affected by Actions of other individual agents
                        * **Note**: Discovered that rational agents should appear to adopt
                        random policies (ambiguous prescription for Actions)
                * Other
                    * Satisficing
                        * Definition: "Good enough" decisions rather than optimally
                        calculated decisions more accurately describe human behaviour 
                    * Optimisation of Sequence Operations
                        * Definition: Rational agent decisions when multiple Actions in 
                        sequence result in payoff
            * Neuroscience
                * Brain
                    * Thought, Action, Consciousness enables Minds - 
                    based on collection of neurons
                        * Mysticism Theory - Alternative theory whereby mind operates in
                        mystical realmn beyond physical science
                    * Cognitive Functions Segregated
                        * Speech production (left hemisphere)
                    * Neuronal Structure
                        * Electrochemical signals sent via neuron 
                        junctions to control brain activity and learning
                        * Information processing in outer layer of brain
                        using tunnels of tissue 0.5mm dia / 4.0mm depth 
                        containing 20,000 neurons
                        * **Unknown** is how damaged functions adopted by others
                        * **Unknown** is how individual memory stored
                        * Note: Single-cell neuron activity artificially stimulated 
                        (electrically, chemically, optically) for recording
                        of neuron I/O mapping relating to cognitive processes
                    * Sensory Signals
                        * Publish/Subscribe mapping between brain areas
                        and bodily parts (control output or sensory input 
                        received)
                        * Note: Multiple maps apparent in some creatures
                        that change every few weeks
                        * Note: Brain activity images using EEG and MRI
                * Research Applications
                    * Math Models applied to nerve cells (neurons) 
                    to study nervous system
            * Psychology
                * Behaviourism
                    * Achievements: Discoveries in some animals but less with humans
                    * Human Behaviour Experiments
                        * Subjective methodology 
                            * Introspective data is subjective 
                            (unlikely experimenter disagree with own theory)
                        * Note: Objective methodology applied to humans
                    * Animal Behaviour Experiments
                        * Objective measures of stimulus given and resulting actions/response
                            * Lack of introspective data
                * Cognitive Psychology
                    * Definition: Brain viewed as a CPU
                    * Knowledge-based Agent Info Processing Steps
                        * Translate stimulus to internal representation
                        * Manipulate internal representation using 
                        Cognitive Processes to achieve new internal
                        representation
                        * Translate back into Action
                * Computer Models should be like Cognitive Theory and
                address psychology of the following by describing information
                processing mechanism for cognitive function implementation:
                    * Memory 
                    * Language
                    * Logical Thought
            * Computer Engineering
                * Future power increases from mass parallelism
                * AI work has pioneered ideas used in computer science
                (i.e. object-oriented programming, etc)
            * Control Theory and Cybernetics
                * Control Theory Tools
                    * Calculus
                    * Matrix Algebra
                * Control Theory Systems
                    * Systems described by "fixed" sets of continous variables
                * Artifact (computer) Systems
                    * Self-regulating feedback control system machines (modify behaviour in response
                    to env changes). i.e. submarine, thermostat
                * AI
                    * Escapes limitations (NOT "fixed")
                    * Logical Inference and Computation Tools solve problems in:
                        * Language
                        * Vision
                        * Planning
                * Purposive Behaviour
                    * Regulatory mechanism that minimises error toward goal state
                * Intelligence AI Source
                    * Homeostatic devices with feedback loops creating stable
                    adaptive behaviour
                * Stochastic Optimal Control
                    * Goal of system designs that behave optimally 
                    (maximising objective function over time)
            * Linguistics
                * Behaviourist Approach - to Language Learning
                    * Ignores Creativity in language (i.e. child makes sentences)
                * Syntactic Structures & Chomsky's Theory
                * Natural Language Processing (aka Computational Linguistics)
                    * Understand sentence structures, subject matter, and context 
                    to understand a language and use Knowledge Representation to convert
                    into form a computer may reason with
                    
### History of AI

up to page 18



## Python Links

* Python `zip, map, lambda` https://bradmontgomery.net/blog/pythons-zip-map-and-lambda/
* Iterate Dictionary/Hash: `for k, v in values.items():`
* Iterate Array: `for val,i in enumerate(range(len(a))):`
http://www.diveintopython.net/file_handling/for_loops.html
                    `values[val]`
* Iterate over String: `[i for i in k]`
* Append to Array: `elim_box.append((k, v))`
* Replace values in string: `removed = original.replace("M", "")`
http://www.petercollingridge.co.uk/book/export/html/362

## Extra Curricular

### Code links

* AIMA textbook example algorithms https://github.com/aimacode/aima-python
    * lecture 6 (right ahead of project 2) and from AIMA only had to cover 5.1-5.4 specifically references DFS and iterative deepening from earlier chapters
* Github projects

https://github.com/udacity?utf8=%E2%9C%93&q=aind
https://github.com/udacity/artificial-intelligence
https://github.com/danainschool/aima-python
https://github.com/udacity/AIND-Isolation
https://github.com/udacity/AIND-Planning
https://github.com/ltfschoen/AIND-Sudoku
https://github.com/ltfschoen/hmmlearn

### Python requirements

* itertools, list comprehensions, tuples, function with multiple return values, default values as function params, functions as first class citizen (pass functions like variables), creating classes
* Anaconda
* Python 3
* Courses preparation https://www.udacity.com/course/design-of-computer-programs--cs212

### OpenAI

https://github.com/openai
https://openai.com/blog/

### Research Papers in AI

https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap
https://github.com/terryum/awesome-deep-learning-papers?files=1
http://www.arxiv-sanity.com/
* Summaries already done https://docs.google.com/spreadsheets/d/1xej5Nca2xUUtrZ1GCyPjFMqI9ZgNq_OhgnTxOOMQ2js/edit#gid=404493967
* DL related (Jack Clark at OpenAI to crowdsource via dhruvp): https://docs.google.com/spreadsheets/d/1MT8LRuEn3xdJ7j6_XQjRZwV_fRmrpjaWwZKGk2Dqm4k/edit#gid=0

### Competitions in AI
* General AI Challenge

https://www.general-ai-challenge.org/active-rounds
https://discourse.general-ai-challenge.org/

### Convert Markdown to PDF
* https://gitprint.com/

### Reading - Python

* Book "Data structures and algorithms in Python"
    * Recursion chapter, Tree/graph algorithms
* Official Python Docs - itertools and other Python packages
* AI Overview 
    * DARPA https://youtu.be/-O01G3tSYpU
* AIMA book
    * Chapters http://aima.cs.berkeley.edu/2nd-ed/newchap11.pdf
    * Code examples for each of the algorithms https://github.com/aimacode
* Python e-books:
http://www.diveintopython3.net
https://learnpythonthehardway.org/book/
http://www.deeplearningbook.org/

* MIT AI course (similar topics to AIND) https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/

## Week 0
* Kick-off event https://youtu.be/z7V7Ri6P_dA
* About
https://www.udacity.com/course/artificial-intelligence-nanodegree--nd889

## Week 1
* Guides
    * Sebastian Thrun - Udacity co-founder
        * Key takeaways:
          - Reinforces that AI surpassing human efficiency for repetitive tasks
          - Highlights self-driving car applications, benefits (i.e. safety), and modern suppliers
          - Identifies need for multi-tasking AI platform
          - Suggests students to build innovative experiments and forward to Sebastian (i.e. using Arduino)
        * Q&A (Starring Luke Schoen)
            * https://youtu.be/AijSw9Q3oas
            * Initiative of re-writing or summarising research papers so more easily
            understood by layman by using visuals and animations like Udacity uses
                * TODO - Central repository that unifies papers into a single site
                    * Interactive tool where can play with parameters with different outputs
                    * i.e. create site with all animations associated with Artificial Intelligence
                    (Udacity would like to work with people involved)
            * Cyc http://www.cyc.com/
    * Peter Norvig - Google - AI book author
    * Thad Starner - Georgia Tech - Contextual Computing & Google X researcher
    * Arpan Chakraborty - Georgia Tech - Computer Vision - AI research
    * David Joyner - Georgia Tech - AI lecturer - applies AI to tools
    * Dhruv Parthasarathy - AI Program director
    * Lisbeth Ortega - Community Manager
    * Kathleen Mullaney - Career Services
    * Rachel Higgens - Career Services
    * Trinh Nguyen - Career Services - Program Coordinator
    * Kirsten Bailer - Career Services - Employer Relations
    * Luis Serrano - Instructor/Course Developer
    * Dean Webb - Mentor Online
    * Shelly - provides Quizzes during course
    * Help: ai-support@udacity.com OR Chris LaPollo (outside enrolment)

* Project 1 - Sukoku AI-agent
    * Build and code AI-agent solves any Sudoku problem
        * Learn 2x AI techniques
            * Constraint Propagation
            * Search
        * Advanced Sukoku strategy to improve agent speed
    * Based on blogpost http://norvig.com/sudoku.html

* Project 2 - Game playing AI-agent
    * AI-agent plays games and defeats opponents in isolation
        * Learn 2x AI Strategies (used by Google's Go agent AlphaGo)
            * Minimax
            * Alpha-beta pruning
    * AI-agent guides pacman through maze faster and eats food efficiently
        * Learn 2x AI strategies
            * Breadth-first search
            * Depth-first search
            * A-Start search
* Project 3 - Planning AI-agent          
    * AI-agent plans optimal route transport cargo from A to B (used in logistics)
* Project 4 - Natural Language Processing ??? AI-agent
    * AI-agent automatically reads sign language chars and convert to English
        * Learn 1x AI strategy
            * Hidden Markov Models

* Deadlines
    * Deadline means submitted and Reviewed
* Code Reviews - Unlimited
* Mentor - Leadership, Guidance, SME
    * Weekly checkin so track.
    * Help set learning goals.
    * Guide to supplementary resources when get stuck
    * Respond to any questions about the program
* Office Hours https://calendar.google.com/calendar/embed?src=knowlabs.com_h7k8bibekume01f054v7ns8v1o%40group.calendar.google.com
    * instructors, content creators and other Udacity staff to answer questions about the content and projects
* Forums - Tech Ques https://discussions.udacity.com/
    * AIND Term 1 Categories - https://discussions.udacity.com/categories
        * **Projects** category – post anything specific to projects in respective sub-categories there
        * **Announcements** specific to cohort including reminders and additional resources
        * **Cafe** interact with your peers
* Community - Online/Offline Study Groups arranged by Udacity 
* Career Services - http://career-resource-center.udacity.com
    * Udacity partners
        * Google, IBM Watson, Accenture, AT&T, Priceline.com, Nvidia, Mercedes-Benz, Amazon
    * Career Path - Location, etc
    * Career Development in Classroom
        * Resume updates
        * Udacity Reviews for free
* Other Students -
    * Sample Search https://www.google.com.au/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=code+solution+naked+twins+sudoku+aind+github
    * Solution to Sudoku Naked Twins - https://github.com/vxy10/AIND-Sudoku/blob/master/solution.py
    * https://github.com/Kiyo921/AI-sudoku
    * Project 2
        * https://github.com/philferriere/aind-projects
        * https://github.com/bdiesel?tab=repositories
        * https://github.com/ziyanfeng/AIND-Isolation
        * https://github.com/davidventuri/udacity-aind
        * https://github.com/mohankarthik?tab=repositories
        * https://github.com/mohankarthik/AIND-P2-Isolation
        * https://github.com/mohankarthik/AIND-L1-Pacman
        * https://github.com/mlgill?tab=repositories
        * https://github.com/mlgill/AIND-Isolation
        * https://github.com/mlgill/AIND-Planning
        * https://github.com/mlgill/AIND-Simulated_Annealing
        * https://github.com/morganel?tab=repositories
        * https://github.com/morganel/AIND-Isolation-1
        * https://github.com/morganel/AIND-Planning
        * https://github.com/madhavajay/nd889
        * arunkv
            * https://github.com/arunkv/AIND-Planning/commits/master

        * sumitbinnani

* Chat - Slack https://artificial-intelligence.udacity.com/
    * ai-nd.slack.com
    * LinkedIn Links https://docs.google.com/spreadsheets/d/1kdiF8weRpmaF91xqmeZ27tqf7IPgWqhpYxeynDb--SA/edit#gid=0
        * Intro
        @channel Hello everyone. My name is Mike Salem and I am the Nanodegree Service Lead for AI. I want to welcome each and every one of you to Artificial Intelligence!
        
        For introductions:
        1. Your name (first name is OK)
        2. Where are you from / located
        3. What's your background / have you taken any AI courses in the past?
        4. Why did you sign up for the AIND? What would you like to learn?
        5. (Optional) Do you have any cool AI related things you would like to show off? be creative. 
        It could be a github account, a youtube videos, etc.
        
        We want your experience at Udacity to be the best possible. So please chat with other people in the channel, make friends, and most importantly learn and have fun!
* Bug Reporting https://waffle.io/udacity/aind-issue-reports

* IntelliJ Python Debugger Setup

Find Miniconda Python:
$ which python
/Users/Ls/miniconda3/bin/python

File > Project Structure > Platform Settings > SDKs > + > Python SDK > /Users/Ls/miniconda3/bin/python

File > Project Settings > Project > Project SDK > Python 3.6.0 (~/miniconda3/bin/python...)

Run > Debug > + > Python
- Name: Sudoku
- Script: /Users/Ls/code/aind/term01/lesson01/function.py
- Use Specified Interpreter: Python 3.6.0 (~/miniconda3/bin/python...)

* Lesson 1 - Anaconda
    * About - Version management (specifically built for data science)
    * About Conda
        * Dfn (Conda) - Package manager (install library/software) and Virtual Environment manager (similar to `virtualenv` and `pyenv`)
            * Note: `pip` is the default Python package manager (general packages)
            * Note: `conda` is Python package manager (focus on data science packages), not specific to just Python packages
        * Dfn (Anaconda) - Large software distribution that comes with Conda, python, and 150 scientific packages (data science) and dependencies
            * i.e. comes with: Numpy, Scipy and Scikit-learn compiled with the MKL library, speeding up various math operations
        * Dfn (Miniconda) - Small software distributin comes with Conda, python (install required packages manually)
    * Usage of Conda
        * Conda Documentation https://conda.io/docs/using/index.html
        * Cheat Sheet https://conda.io/docs/_downloads/conda-cheatsheet.pdf
        * Cheat Sheet Advanced http://know.continuum.io/rs/387-XNW-688/images/conda-cheatsheet.pdf?mkt_tok=eyJpIjoiWm1Sak56UmpZMlZsTW1aaSIsInQiOiJOc3Rud2NraTI4ODlzVmJBSnllZ0JJWk1ZSlhLbE5QOEhZUWRMMDY2NWUrNGZxalRnSXNcL2NOM3h2UVFJN0ZJV0YybGpGOFNrdEJWa0Rnd1hlRnFzXC9wdUJcL1c1bkxsWVZIT1JzXC9vRGwwSmNvREpGXC9UYk9LV01UcjV6b1Z3TlN2In0%3D
        * Another https://leifengtechblog.files.wordpress.com/2016/01/conda-cheatsheet.pdf
        * Steps
            * Create new environement for each project (separate package versions so no version conflicts)
                `conda create -n tea_facts python 3`
            * Delete env
                `conda remove -n tea_facts --all` (or `conda env remove -n tea_facts`)
            * Activiate environment
                `source activate tea_facts`
            * Deactivate environment
                `source deactivate tea_facts`
            * Show default packages installed
                `conda list`
            * Show non-Python packages
                `conda search --canonical  | grep -v 'py\d\d'`
            * Update environment with new packages (i.e. work with t-data, visualisations, code development)
                `conda install numpy pandas matplotlib`
                `conda install jupyter notebook`
            * Update a package
                `conda update ____`
            * Update all packages
                `conda update --all`
            * Find a package
                `conda search ___`
            * Export list of packages in environment to file for easy loading dependencies by others (similar to `pip freeze > requirements.txt`)
                `conda ` 
        * Other
            * Help `conda install --help`
    * Installation of Conda
        * Anaconda 
            * Download https://www.continuum.io/downloads
            * Cheat Sheet of Installation https://docs.continuum.io/_downloads/Anaconda_CheatSheet.pdf
        * Miniconda https://conda.io/miniconda.html
            * https://github.com/conda/conda
            * Download v3.5
            * Run with `bash Miniconda3-latest-MacOSX-x86_64.sh`
            * Check installation location
                `which conda` => /Users/Ls/miniconda3/bin/conda
                `conda list`
                
                ```
                 # packages in environment at /Users/Ls/miniconda3:
                 #
                 cffi                      1.9.1                    py36_0  
                 conda                     4.3.11                   py36_0  
                 conda-env                 2.6.0                         0  
                 cryptography              1.7.1                    py36_0  
                 idna                      2.2                      py36_0  
                 openssl                   1.0.2k                        0  
                 pip                       9.0.1                    py36_1  
                 pyasn1                    0.1.9                    py36_0  
                 pycosat                   0.6.1                    py36_1  
                 pycparser                 2.17                     py36_0  
                 pyopenssl                 16.2.0                   py36_0  
                 python                    3.6.0                         0  
                 readline                  6.2                           2  
                 requests                  2.12.4                   py36_0  
                 ruamel_yaml               0.11.14                  py36_1  
                 setuptools                27.2.0                   py36_0  
                 six                       1.10.0                   py36_0  
                 sqlite                    3.13.0                        0  
                 tk                        8.5.18                        0  
                 wheel                     0.29.0                   py36_0  
                 xz                        5.2.2                         1  
                 yaml                      0.1.6                         0  
                 zlib                      1.2.8                         3
                ```
        * List envs

        `conda info --envs` (or `conda env list`)
        
        ```
         # conda environments:
         #
         root                  *  /Users/Ls/miniconda3
         ```
         
         * Clone the root env
         
         `conda create --name rootclone --clone root`
         
         * Create env with specific Python version 
         `conda create --name aind-3.6.0 python=3.6.0`
         
         * Checkout the aind env
         
         `source activate aind`
     
         * Save env (incl packages and Python version for sharing). 
          Also include a pip requirements.txt file using `pip freeze` for people not using conda
         
         `conda env export > environment.yaml`
         `pip freeze > requirements.txt`
         
         * Load env from YAML env file `conda env create -f environment.yaml`
           
    * Installation of Spyder IDE
        * https://github.com/spyder-ide/spyder
        
        * Create subenv with Spyder IDE, update to latest versions
            `source activate root`
            `(root) $ python --version` ==> 3.6.0
            `(root) $ conda upgrade conda`
            `(root) $ conda upgrade --all`
            `(root) $ conda create -n spyder python=3.6.0`
            `(root) $ source activate spyder`
        * Install Spyder IDE in subenv
            `(spyder) $ conda install qt pyqt`
            `(spyder) $ conda install spyder`
            `(spyder) $ conda upgrade --all`
        * Run Spyder IDE
            `(spyder) $ spyder`
            
    * Python 3 vs Python 3
        * Usage of `print` from Python 2 instead of Python 3's print function (surround with `print("a","b")` not `print "a" "b"`)
        * Use new print function in old Python 2 code by importing `from __future__ import print_function`

    * Env variables
        * https://conda.io/docs/using/envs.html
    * Pre-load with AI Nanodegree Python environment
        * Download aind-environment-unix.yml
        * Load `conda env create -f /Users/Ls/Downloads/aind-environment-unix.yml`
        * Switch to AIND env `source activate aind`
    * Install PyGame for visualisations of AI programs for portfolio sharing
        `brew install sdl sdl_image sdl_mixer sdl_ttf portmidi mercurial`
        `pip install pygame`
    * Get PyGame library directory
        ```
        >>> import site;print(site.getsitepackages())
        ['~/miniconda3/envs/aind/lib/python3.6/site-packages']
        ```
    * Within IntelliJ, go to Project Structure (CMD+;)> Platform Settings > SDKs, and Add
    `~/miniconda3/bin/python` as a Python 3.6.0 SDK home path, then add to this an additional Classpath
     of `~/miniconda3/envs/aind/lib/python3.6/site-packages`
    * Within IntelliJ, go to Project Structure > Platform Settings > Modules
    
* Project 1 - Suduko
    * About Sudoku http://www.conceptispuzzles.com/?uri=puzzle/sudoku/rules
        * Given: 9x9 grid partially completed
        * Objective: Fill grid with missing digits so each row, each col, and
        each of 9 principal 3x3 sub-squares contains all digits 1-9
        * Extras:
            * Strategies other than Naked Twins like:
            (Standard + Diagonal Sudoku)
    * Goal
        * Build an AI-agent that solves all Sudoku problems and 
        provides intro to techniques: Constraint Propagation, and Search
    * Note: **AI**
        * Composed of combining many simple ideas to solve complex problem 
    * Dfn: **Constraint Propagation**
        * e.g: Extract max info from applying simple Local Constraints (i.e. to each square) 
        to iteratively narrow search space of possible solutions
        * Risk: Never solve every puzzle (incl. hard ones)
        * Example: Elimination of possible values for each peer within each square
        here http://norvig.com/sudoku.html states that even using 
        **Naked Twins** strategy we never know if we can solve every puzzle (even hard ones)
    * Dfn: Search
        * e.g. Given two or more possibilities available, branch out and create whole
        tree of possibilities, then traverse tree and consider all until find solution
        * Risk: May take forever to run
        * Example: 
            * Option 1: **Brute Force** Approach: Single machine instruction http://norvig.com/sudoku.html
            (i.e. try all possibilities, takes a long time)
            * Option 2: **Constraint Propagation** (Constraint satisfaction problem): Multiple machine instructions each processing
            multiple possibilities (i.e. eliminate possibilities on each try, so not need try all possibilities)
                * https://en.wikipedia.org/wiki/Constraint_satisfaction
                * https://en.wikipedia.org/wiki/Search_algorithm
                * **Depth-First Search** Algorithm:
                    * Dfn: Depth First Search (DFS) algorithm traverses a graph in a depthward motion
                    and uses a stack to remember to get the next vertex to start a search, 
                    when a dead end occurs in any iteration
                    * Check not already found solution or contradiction
                    * Select unfilled square and try possible values until find successful value, 
                    and for each possible value check if with that possible value we can successfully
                    search for a solution.
                    * Note: This is a **recursive search** since we recursively consider all possibile
                    outcomes for a value before trying a different value
                    * Note: Create new copy of values for each recursive call to search so each branch
                    of the search tree is independent and does not confuse other branches
                    (hence may be more efficient to implement set of possible values for a Sudoku square as 
                    a string, rather than a Set or a List)
                * **Search Implementation**
                    * **Variable Ordering** - i.e. choice of which square to try first
                        * **Minimum Remaining Values Heuristic** - i.e. choose a square with min. qty of possible values
                            * i.e. if choose square with 7 possibilities we expect to guess wrong with probability
                            of 6/7, whereas if choose square with 2 possibilities we expect only 1/2 (50%) probability
                    * **Value Ordering** - i.e. choice of which digit to try for first Sudoku square
                    * **Randomize Ordering Heuristic** - Randomise the value ordering may avoid any deadly combinations of value choices
                    that may take 10,000 times longer to find a contradiction
                    * **Least-Constraining Value Heuristic** - Chooses the first value that imposes the least 
                    constraints on peers
    * Problem: 
        ```
        . . 3 |. 2 . |6 . .
        9 . . |3 . 5 |. . 1
        . . 1 |8 . 6 |4 . .
        ------+------+------
        . . 8 |1 . 2 |9 . .
        7 . . |. . . |. . 8
        . . 6 |7 . 8 |2 . .
        ------+------+------
        . . 2 |6 . 9 |5 . .
        8 . . |2 . 3 |. . 9
        . . 5 |. 1 . |3 . .
        ```
    * Solution: '483921657, 967345821, 251876493, 548132976, 729564138, 136798245, 372689514, 814253769, 695417382'
        ```
          1 2 3  4 5 6  7 8 9
        A 4 8 3 |9 2 1 |6 5 7
        B 9 6 7 |3 4 5 |8 2 1
        C 2 5 1 |8 7 6 |4 9 3
          ------+------+------
        D 5 4 8 |1 3 2 |9 7 6
        E 7 2 9 |5 6 4 |1 3 8
        F 1 3 6 |7 9 8 |2 4 5
          ------+------+------
        G 3 7 2 |6 8 9 |5 1 4
        H 8 1 4 |2 5 3 |7 6 9
        I 6 9 5 |4 1 7 |3 8 2
        ```
    * Steps:
        * If box has a value, then all boxes in same row, same column, or same 3x3 square cannot have same value
        * If only one allowed value for given box in row, column, or 3x3 square, then the box is assigned that value
        * Naming Conventions
            * Rows and Columns
                * Rows labelled A, B, C, D, E, F, G, H, I.
                * Columns labelled 1, 2, 3, 4, 5, 6, 7, 8, 9
            * 3x3 squares will not be labelled
            * Boxes, Units and Peers (name important elements created by rows and columns)
                    * Boxes - Individual squares at the intersection of rows and columns (i.e. 'A1', 'A2', ..., 'I9')
                    * Units - Complete rows, columns, and 3x3 squares. Each unit is a set of 9 boxes, there are 27 units total.
                    * Peers - All other boxes associated with a particular box (such as 'A1'), that belong to the same unit 
                    (i.e. belong to the same row, column, or 3x3 square)
                    (i.e.  Peers of 'A1': row: A2, A3, A4, A5, A6, A7, A8, A9 
                                          column: B1, C1, D1, E1, F1, G1, H1, I1 3x3 
                                          square: B2, B3, C2, C3 (since A1, A2, A3, B1, C1 are already counted).
                    * Note: Each box has 20 peers
        * Elimination Strategy:
            * Eliminate possible values for a box by analysing peers
            * If a box has a value assigned, then none of its peers may also have its value
            * Update grid with possible values for each box
            * Use to compute `grid_values()` so each box value represents all the available 
            values for that box. i.e. i.e. box 'B5' would have value '47' 
            (because 4 and 7 are its only possible values).
            Starting value returned for every empty box will be '123456789' instead of '.'
        * Only Choice Technique:
            * If one of the possible values in any one of the unsolved boxes of a
            unit/square that is not also one of the remaining values in the other
            unsolved boxes, then it is the Only Choice for that box
        * Search Strategy:
            * Example Usage:
                * Game Playing
                * Route Planning (to find solutions efficiently)
                * Alpha Go https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf
                    * Selection
                    * Expansion
                    * Evaluation
                    * Backup
                * **Depth First Search Algorithm** Steps:
                    * Start from root, traverse the tree always in the most-left-first
                    fashion, and when reach end leaf, move to next branch
                    * Step 1: Pick an unsolved box that has the fewest possible numbers
                    to try out i.e. 89 (less work), and create a
                    Tree of possibilities to try out
                    * Step 2:
                        * Step 2a: Try the first branch's possible value of 8, using
                        Constraint Propagation
                        * Step 2b: If not done after first branch's first level, then choose
                        from that unit/square another box with fewest possible numbers
                        (i.e. 159), and create 3x 2nd level branches to Try solving with
                        using 8 with each of its values (i.e. 1, 5, and 9) using
                        Constraint Propagation
                        * Step 2c: Repeat with more levels as required

                    * Step 3: Try for remaining branches
                    * Step 2: If Step 1-3 results in no solution, repeat for another one
                     of its possible values
                    * Note: 4x more work, but each step gets easier
        * Constraint Propagation:
            * Using local constraints in a space (i.e. Sudoku square constraints)
            to reduce search space. Each constraint that is enforced introduces
            new constraints for other parts of board that helps reduce remaining
            possibilities
                * Example Usage:
                    * Map Coloring Problem: Prevent two adjacent map segments from having same color
                    * Crypto-Arithmetic Puzzles: Where each unique letter represents a digit
                    in a calculation (i.e. TWO + TWO = FOUR), and no number starts with a 0.
                    Goal is find mapping from letters to digits to satisfy equations by
                    using constraints imposed by the equation to create an AI-agent to solve the
                    proble via Contraint Propagation
                    * Sudoku:
                        * Options:
                            * Constraints: Elimination
                                * Pick solved box, and apply the Eliminate function to
                                remove the value of the box from the possibilities of its peers
                            * Constraints: Only Choice
                                * Pick a Unit, if there is one value in the unsolved boxes
                                that is only allowed in one of the boxes, then assign it to
                                the box
                            * Constraints: Search
                                * Use Depth First Search algorithm to solve harder Sudoku problems
                                 using recursion and calling function that uses Constraint Propagation
                                 (i.e. Elimination and Only Choice techniques)
                            * Note: **Constraint Propagation Technique
                            (using Elim & Only Choice)** Applied
                                * Step 1: Pick a solved box and apply Eliminate function
                                * Step 2: With the more complete grid, pick a Unit and apply
                                Only Choice function
                                * Step 3: Repeat Step 1 for a different solved box
                                * Step 4: Repeat Step 2
                                * etc
                                * Solution ?? Or Stuck ?
                                * **Important Note:** May only handle EASY puzzle
        * Naked Twins Technique (extension) http://www.sudokudragon.com/tutorialnakedtwins.htm
            * Given a Sudoku puzzle partially completed with some boxes with possibilities for their
            values. If there are two boxes in the same row or column that both contain the same two values
            i.e. 23 and 23, then we know for sure that 2 and 3 can only occur in those boes in that
            unit (column 3), so:
                * Remove 2 and 3 from possible values in that column
        * Diagonal Sudoku (by modifying the algorithm)
    * Project 1:
        * Added Python Debugging functionality from main.py file (see my Gists on GitHub)
        * Added assertions
        * Get PyGame library directory
            ```
            >>> import site;print(site.getsitepackages())
            ['/Users/Ls/miniconda3/envs/aind/lib/python3.6/site-packages']
            ```
        * Code Review Comments:
            * Logging with default level ERROR could be added to debug the code.
            Logs can also help to understand the algorithms.
            See https://docs.python.org/3/howto/logging.html
            * Assert statements could be used https://wiki.python.org/moin/UsingAssertionsEffectively
        * Utility methods in separate file
    * MyPy - Static Type Checking
        * Install MyPy `python3 -m pip install mypy`.
        * Install Typing `python3 -m pip install typing`.
        * Import Typing `import typing; from typing import *`,
        * Apply MyPy static type checking to an existing function where expect warnings
            `def solve(grid: str) -> None:`
        * Run MyPy Linter with `mypy solution.py`
        * Apply MyPy with expected types `def solve(grid: str) -> typing.Dict:`

        * Installation http://mypy.readthedocs.io/en/latest/getting_started.html#getting-started
            `python3 -m pip install mypy`
            * Also installed `typing`, even though at this website it says it's only necessary
            when running old Python 2 http://mypy.readthedocs.io/en/latest/faq.html
                `python3 -m pip install typing`
                * And then added to top of file:
                    `import typing; from typing import *`
                * Implemented into function with:
                    `def solve(grid: str) -> typing.Dict:`
            * Run MyPy Linter
                `mypy solution.py`
        * Basic Summary http://mypy.readthedocs.io/en/latest/basics.html
        * Note:
            * Example built-in types: http://mypy.readthedocs.io/en/latest/builtin_types.html
        * Run MyPy Linter that performs type checking of program without running it and provides warnings
            `mypy program.py`
            ```
            PySudoku.py:1: error: Cannot find module named 'pygame'
            PySudoku.py:1: note: (Perhaps setting MYPYPATH or using the "--ignore-missing-imports" flag would help)
            PySudoku.py:3: error: Cannot find module named 'SudokuSquare'
            PySudoku.py:4: error: Cannot find module named 'GameResources'
            PySudoku.py:65: error: Name 'main' is not defined
            ```
        * Extend MyPy Linter to perform type checking of Libraries that have Stubs that
         are skeletons of public interface of library classes, variables, functions and their types
         but with Dummy function bodies (i.e.
         those incorporated in the Typeshed project from Python Built-ins and Standard Library)
            * Type inference of uses Stubs for definitions
                ```
                # Sample code in .py file
                x = chr(4)

                # Sample MyPy Library Stub in .pyi file (ellipsis used for function body)
                def chr(code: int) -> str: ...
                ```
            * Create Custom Stubs https://github.com/python/mypy/wiki/Creating-Stubs-For-Python-Modules
                * Write Library Stub in .pyi file within either same directory as the Library Module OR
                in specific Stub directory (i.e. myproject/stubs) and set environment variable
                pointing to `export MYPYPATH=~/work/myproject/stubs`
                * Use subdirectory with `__init__.pyi` for packages
                * In directories with both `.pyi` and `.py` the `.pyi` file takes precedence
        * Contribute Custom Library Stubs created into the Typeshed Repo
        * Static Type Checking (instead of Dynamic Type Checking)
            * Type Annotations added to functions (type checker reports type errors within function)
            * Apply return type as None by default so statically typed context results in type check error
                ```
                def greeting(name: str, prefix: str = 'Mr.') -> None:
                     return 'Hello, {} {}'.format(name, prefix)
                ```
            * Use `typing` module to use type definitions in statically typed code
                ```
                from typing import Iterable

                def greet_all(names: Iterable[str]) -> None:
                    for name in names:
                        print('Hello, {}'.format(name))
                ```
        * Previously Dynamic Type Checking
            * No type checking annotation in function signatures
    * Lesson 4: Intro to AI
        * **Optimisation** of Navigation and Route Finding Domain
            * Computationally hard problems
                * Find shortest path on map from A to B
                    * Apply graph of city nodes
                    * Connect city nodes by road edges
                * Option 1: **Breadth-First Search**
                    * Inefficencies
                        * Considers all possible paths out of city until
                    reaches destination via one of them
                        * Only considers major roadways
                    * Issues
                        * Intractable space of possible paths to search
                        if consider smaller streets/alleys
                * Option 2: Heuristic to Manually Solve (i.e. AI for smart idea, a rule,
                function, or constraint that informs an otherwise
                Brute-force Algorithm to act more optimally)
                    * Consider major roads
                    * Defer considering major roads going too far
                    * Prioritise paths that take closer to destination
                * Option 3: **A\* Search Algorithm**
                    * Draw straight line path between source and destination
                    * Priorities how our algorithm considers and evaluates
                    different possibilities by try testing
                    (using Direct Distance Heuristic)
                    all possible next steps from current position
                    so new direct distance changes (prevent unnecessary
                    searching and converges on optimal direction quickly).
                    Choose Most Promising Choice with smallest
                    direct distance as next step (but may not turn out to be
                    Optimal). Store each other path for later consideration.
        * **Intelligent Agent**
            * **Dfn**: Actions taken to Maximise its expected Utility given desired Goal
            * **Comparison of Agents**
                * **Rational Behaviour** extent - requires Optimal Behaviour
                 (regardless of what Cognitive Mechanism used),
                 however, the Optimal Solution is hard to achieve (i.e. Sudoku problem, etc),
                 but AI-agents are not always expected to perform Optimally
                * **Constraints** faced by the AI-agent must also be considered
                    i.e.
                    * Partially Observable env
                    * Limited computational resources (speed, memory)
                    * Rules of task such as require respond by deadline
                * **Bounded Optimality**
                    * Quantifying Intelligence feasibly and practically
                    * Level of Performance / Bound for AI-agent to meet
                    (i.e. Chess AI-agent to win 60% of time against human chess masters,
                    or Route Finding Algorithm that always find route no more than
                    2km longer than Optimal route)
            * i.e. Anticipate and React to changes in environment (i.e. other drivers)

        * **Game AI-Agent** (Tic Tac Toe)
            * Goal - Design AI-agent to play Game (Tic Tac Toe) using
            Search Strategy (similar to route-finding problem)
            * Step 1
                * Determine Nodes and Edges in the Graph we search
                    * Option A -
                        * Nodes - Each cell
                        * Edges - Connect two if adjacent
                        * INSUFFICIENT - Does not capture enough info on what to do next
                    * Option B -
                        * Node & Edges - Current Player position (row/col Tuple)
                        {0, 1, 2}
                        Note: Two Nodes connectable if row/col differ by 1
                        (i.e. (0, 1) connectable to (1, 0) but not (2, 2)
                        * INSUFFICIENT
                    * Option C -
                        * Opponent Tracking - Track of opponents position too
                        * Node - Tuple (i.e. <(0, 1), (2, 0)>
                        * Edges - Connect if both Player positions differ
                        by max 1
                        * INSUFFICIENT - Since need know which cells already
                        filled, as cannot move into them
                    * CORRECT - Option D -
                        * Node - **Whole Board is Node**, with one Node for every
                         possible arrangement of X's and O's
                        * Edges - Connect two Nodes if there is a valid move
                        that changes the board from one node to another
                            * The edges embody the **Rules** of the game
                                * i.e. only possible to go to finite qty of
                                next states from current state
                                (limits qty of moves need to consider)
                        * SUFFICIENT - Usable for Tic Tac Toe!!! Since will know
                        position of Computer Player and Opponent Player so
                        know what to do next (by Excludes any extraneous details
                        we don't need to solve it)
                        * Note:
                            * 3x3 Board -
                                * First Computer Move - 9 possibilities
                                * First Opponent Move - 8 possibilities
                                * etc
                        * Note: Search Indicators
                            * **Pruning the Search Tree**
                                * Ignorable / Non-Winnable Moves (some moves are wasteful)
                                    * Evaluated Condition - i.e. When enemy has X on same row and column and proposed placement
                            * **Adversarial Search**
                                * **Minimax Algorithm** (maximise chance of win on our turn, whilst enemy trying to
                                minimise our chance of win on their turn)
                                Note: **AI-agent Dfn - Anticipates and plans around expected changes in its environment,
                                including those introduced by other agents**
                                    * Anticipate Enemy Movies (intuition of recognising that enemy is Human also trying to Win)
                                        * Evaluate Condition - i.e. When enemy can make sequence of moves that win after our move
                                            * **Rule Out** certain moves as being BAD removes
                                            potential successors from its consideration
        * **Monty Hall Problem**
            * Given 3x doors, randomly assigned behind each is either car or goat
                * Strategy:
                    * Stay   - Wins 33.33% of time
                    * Switch - Wins 66.67% of time

                * Strategy:
                 1) C G G - better to STAY
                    x 0

                 2) C G G - better to STAY
                    x   0

                 3) C G G - better to SWITCH
                      x 0

                    C G G - never, so counts as 3) probability
                    0 x

                 4) C G G - better to SWITCH
                      0 x

                    C G G - never, so counts as 4) probability
                    0   x
            * **Probability Theory** - captures uncertainty in
            real-world to make sensible decisions when unsure
                * **Prior Probability** - best guess given no other evidence
                    ```
                    P(CarA) + P(CarB) + P(CarC) == 1
                    P(CarA) == 1/3, P(CarB) == 1/3, P(CarC) == 1/3 (aka **Prior Probability**)
                    ```

                    * Equal to 1 since probability of at least one
                    is car is 100% (sum of probabilities equals 1)
                    * So each door has 1/3 probability of car behind it
                * **Move 1** -
                    * Player 1 Select door A
                    * Monty Open door B
                        * OpenB (Bbservation)
                        * **Posterior Probability** - belief after making an Observation
                        ```
                        P(CarB | OpenB) == 0
                        ```
                        * Note: 2x doors still undisclosed, probability of car behind either
                        is NOT 1/2, since Monty (enemy) knowingly opened door with
                        Goat behind it (never reveal the car on this move)
                        ```
                        NOT THIS - P(CarA) == 1/2, P(CarC) == 1/2
                        ```

                        * We are actually interested in Posterior Probability of finding
                         car behind Door C, given that Monty opened Door B, and the implicit
                         Rule that Monty never reveals the car, so
                         express the **Posterior Probability** in terms of other quantities
                         that can compute using **Bayes' Theorem**
                            ```
                            P(CarC | OpenB) ==

                                == (P(OpenB | CarC) * P(CarC)) / P(OpenB)
                                == (Likelihood * Prior) / Marginal Probability

                                Likelihoood:
                                    where P(OpenB | CarC) == 1
                                    since if CarC true, Monty's only option is OpenB,
                                    since OpenA already picked by us (not option to Monty)

                                Prior:
                                    where P(CarC) == 1/3,
                                    which previously computed as 1/3

                                Marginal Probability:
                                    P(OpenB) == Possibility 1 + Possibility 2 + Possibility 3
                                                    where Possibility 1:
                                                        Car behind A with 1/3 possibility, and in this case Monty either:
                                                        OpenB or OpenC, both with 1/2 possibility,
                                                        so Probability of OpenB **given** CarA is 1/3 (i.e. P(OpenB | CarA))

                                                    where Possibility 2:
                                                        Car behind B has 1/3 possibility, but in this case
                                                        Monty would not reveal it, so Probability of OpenB
                                                        given CarB is 0
                                             == P(CarA) * P(OpenB | CarA) +
                                                P(CarB) * P(OpenB | CarB) +
                                                P(CarC) * P(OpenB | CarC)
                                             == 1/3     * 1/2             +
                                                1/3     * 0               +
                                                1/3     * 1
                                             == 1/6 + 1/3
                                             == 1/2

                                 so, updated Bayes' Theorem
                                 == (Likelihood * Prior) / Marginal Probability
                                 == (1          * 1/3  ) / 1/2
                                 == 2/3
                                 (so probability that the car is behind the other closed door has
                                  increased to 2/3)
                            ```
                * Note: **Empirical Evaluation** (by characterisation of given situation
                coupled with a formal method helps make decisions in face of **Uncertainty**)
                shows that consistent switch strategy wins 2/3 of time

        * **Intelligence Dfn**:
            * Ability to produce Reasonable behaviour while dealing with different
            sources of complexity
                * Intelligent Objects: Thermos, Plant, Siri, Rock, Arpan
                * E.g: If a person you know was not human but acted same,
                 you probably wouldn't tell the difference

            * Note:
                * **Intelligence** often used to label this we cannot explain,
                * **Artificial General Intelligence Dfn** (sub-field of AI) deals with goal of:
                    * Ability to replicate complex nature of human thinking that accomplishes
                     a variety of intellectual tasks like humans do
                * **Algorithms** or **Formulas** used to label things we CAN explain
                * Philosophically this implies that:
                    * Anything we design can never be intelligent, as we would know how it works
                    * Humans assume they are intelligent now, as we do not fully understand how brains work,
                    but when we do, will we not consider ourselves intelligent anymore?
                * **Intelligence** - **must not be contingent on how we Perceive something**
                * **Intelligence** **must be a Property that emerges from the system itself**
                * **Intelligence** - **should be defined as the ability to do something
                                     with a decent rate of success within the Context of a task
                                     so then can say you are Intelligent at solving Sudoku puzzles**

                 (quote by Luke Schoen i.e. Our judgement of the "truth" of something, similar to our
                  judgement of the "intelligence" of something, should not be contingent on how we "perceive" that
                  something. Instead our judgement should be based on assessing "properties" that
                  emerge from the "source" of that something itself, and should be defined based
                  on that somethings' "success rate" at being able to be "truthful" within a defined
                  "context" such as a task)
                * Humans are **Intelligent** at doing Multiple tasks (many things)

        * **Agent Design (i.e. AI-agent)** (i.e. aka Intelligent Systems)
            * AI-agent Dfn:
                * Cartesion View:
                    * Software only is the AI-agent, whereas the Hardware (i.e. sensors) is considered
                * High-level View:
                    * Entire robot is the AI-agent, all components directly reliably accessible/controllable
            part of the Environment
            * Example:
                * Automatic Vacuum - i.e. specification

                    * **Environment Understanding (Domain of Task / Problem)**
                        * room operating in, floor, walls, objects to avoid
                        bumping into, except objects not of concern
                    * **State** stored is only items necessary to complete task
                        * current position
                        * orientation while making actions
                        * places already been before
                        * obstacle location map (but they may move over time)

                    * **Goal State** stored - result the AI-agent trying to achieve
                        (i.e. final location(s) acceptable)

        * Agent Interaction with Environment
            * Input
                * **Perception** - Sensing the Properties of Env
                * **Effectors**
            * Internal
                * **Cognition** - Based on Perceived Inputs, the process of:
                    * Reasoning
                    * Decision Making
                    * Action
                * Note:
                    * **Reactive / Behaviour-based Agents**
                        * Simple pre-programmed behaviours and directly associate
                        Actions with Perception (bypassing process)
                        (i.e. automated rubbish bin that senses something infront of it)
                * **Classification of Agents**
                    * Classification Measures: - based on
                        * Processing they do, the kind of
                        * Environment Properties
                        **Environment States** components they must capture
                            * **Fully Observable** (i.e. Tic Tac Toe where whole board viewable)
                            * **Partially Observable** (i.e. Battleship game where opponent
                                positions hidden)
                            * **Deterministic** (i.e. where **Certain** of Action outcomes)
                            * **Stochastic** (i.e. where **Uncertainty** in Action outcomes)
                            * **Discrete** (i.e. finite qty of States the env may be in)
                            * **Continuous** (i.e. infinite qty of States such as due to
                                            Properties that need to be stored as real numbers)

                            * **Benign** (i.e. where AI-agent is only one taking Actions
                                            that intentionally affect its Goal, i.e other Random
                                            events may be occurring)
                            * **Adversarial** (i.e. where one or multiple AI-agents that can take
                                            Action to defeat its Goal, i.e. competitive games)

                        * e.g.
                            * Playing Poker - Partially Obs, Stoch., Adversarial
                            * Recognition of Hand-written Text - Stochastic, Continuous (since
                                source is physical object with arbitrary and unpredictable motions)
                            * Driving on Road - All except Adversarial
                            * Playing Chess - Advers.

                    * **Layering** simple **Reactive** control
                    in a Hierarchy may achieve complex Behaviour
                    * **Deliberate / Non-Trivial Processing** by agents such as:
                        * **Knowledge-based Agents**
                        * **Game Playing Agents**
                        * **Path Planning Agents**
                        * **Learning Agents**

            * Output
                * **Actions** - Changing the State of Env

    * Lesson 5: Intro to Game Programming
        * Course Outline
            * **Search and Logic and Planning** - Peter Norvig
            * **Probability and Bayesian Networks (BN)** - Sebastian Thrun
            * Other - Thad Starner

        * Goal of Lesson
            * Design a Game AI-agent program smarter than ourselves when playing a
            the limited context of a single real game

        * Main Topics
            * **Adversarial Search** - find optimal moves
            * **Minimax Algorithm** - for **Deterministic Games** determines
                                  best move for any turn in a game
            * **Expectimax Algorithm** - for **Non-Deterministic Games** (Chance games)
                                that considers all possible outcomes and chooses the
                                one with maximum expected return
                                (assuming opponent is making best moves available)
                * **Expectimax Game Tree**
                    * Maximising Nodes - indicated by upward facing triangles
                    * Minimising Nodes - indicated by downward facing triangles
                    * Probabilistic Nodes - indicated by circles with the Chance of
                                        each outcome labelled on branches below it
                    * Evaluation of Tree Approach - evaluate tree from left to right
                    * Note: Game Board Nodes - values may range from -10 to +10
            * **Alpha-Beta Pruning** - helps optimise the Minimax / Expectimax Algorithm to
                                       make AI-agent play faster
                    * Performance Improvements:
                        * Apply to expected max game trees that have finite limits
                        on values the evaluation function returns
                    * Usage: Use the Alpha-Beta Algorithm to mark trees branches that
                      do not need to be visited, to calculate the value of the optimum
                      choice
            * **Evaluation Function** - creation of them for games
            * **Isolation Game Player** - apply knowledge to the Isolation Game (a complex game
            with simple rules)
                * Objective: To not become isolated an unable to move on your turn
                (i.e. be the last player to move)
                * Setup: 5x5 board, 2x players with X and O pieces
                * Movement Rules: Move permitted to any square up/down or diagonal
                from current position, but not through:
                    * opponents position
                    * previously occupied position (i.e. landed upon)
                    * outside game board position
                * https://www.youtube.com/watch?v=n_ExdXeLNTk
                * Note: Hard to estimate chances of winning at start of game,
                 but become more certain as game progresses
                * **Minimax Algorithm** Usage:
                        * Automatic first choice consults **Openning Book - Table of Best First Moves
                        (used for End Games)** for this board config
                        * Discover best moves, i.e.
                            * **Move to positions that are always at least above, or in the
                                position immediately above the opposition position**
                            * **Move to positions with available escape options
                                (unless can block and win on that specific move);
                                and closest to other player position;
                                and avoid some first moves entirely (i.e. top middle or bottom middle);
                                higher odds of winning when make first move on 3x2 (w/h) board
                                with bottom right position blocked off**
                            * Mark (with **Downward Arrow** above) Scores at **Terminal** (Leaf) Nodes
                                * +1 for Win
                                * -1 for Lose
                            * Assumption 0: Maximization of score is being considered from the **Perspective**
                              of the [Computer] (otherwise we'd reverse how we apply Minmax). The [Computer]
                              can only affect the game on its turn.
                              Top of Minimax Algorithm Tree is always a Maximize (**Upward Arrow** Below)
                            * Assumption 1:
                                * Player X [Computer] always tries to maximise its score
                            * Assumption 2:
                                * Player X [Computer] is smart and latest positions are due to them
                                always plays to win trying to minimise Player O's score, so they make
                                bad moves and lose soon)
                            * Mark Assumption 1 (with **Upward Arrow** Below, i.e. [Computer]'s Turns) on Tree,
                              Turns when Player X [Computer] is trying to Maximise the score
                            * Mark Assumption 2 (with **Downward Arrow** Below, i.e. [Human]'s Turns) on Tree,
                              Turns when Player X [Computer] is trying to Minimise the score
                              (i.e. Player O Turns where Player X [Computer] reliant on their Bad Move)


                    * Move to positions that partition the board (fill positions on one side
                    of board to reduce positions to move to to force early end to the game)
                    * Move to positions that try to Trap the opposition
                * Example:
                    * Build a Game Tree (showing all possible movies) based on 2x3 game board
                    * Game Tree used to select moves with best change of winning
                    * Rules:
                        * Unavailable Moves: Any filled spot on game board
                    * Human is X, Computer is O.
                    * Mark Leaf (**Terminal Nodes**) branches where WIN with +1, and LOSE with -1.
                    * Level 0 (Beginning Game Level) - bottom left is unavailable immediately.
                        * Draw Level 0 of **Game Tree** as top of tree

                    * Level 1 [Computer] - 5x possible moves.
                        * Draw Level 1 of **Game Tree** as 5x branches possibilities below Level 0
                    * Level 2 [Human] - 4x possible moves (reduces)
                        * Draw Level 2 of **Game Tree** as 4x branches below EACH Level 1 branch
                    * Level 3 [Computer] - 3x OR 2x OR 1x possible moves (exponential growth)
                                           (some moves BLOCKED by X, or BLINDSPOT)
                    * Level 4 [Human] - 2x OR 1x possible moves (becomes denses)
                    * Level 5 [Computer] - 1x OR 0x possible moves (multiple LOSE possibility)
                    * **Note** Warn [Computer] not to make Level 3 moves that allow
                    Level 4 branch to be one that gives [Human]'s a possible that results in
                    on 0x possible moves for [Computer] in Level 5
                    * Level 6 [Human] - 0x possible moves (i.e. Computer Win if get to this level)

                    * **Minimax Algorithm** Dfn -
                        **Back Propagate possible future wins/losses up game tree** to [Computer] so
                    best possible first move is made
                        * Start Bottom Left Leaf **Terminal Node**
                        * Go Upward and if get:
                            * **Downward Arrow** choose the Min value (from -1 or +1) appearing in Child Nodes
                                (Turn where Player X [Computer] is trying to Minimise their opponents score)
                                * Note that **since [Computer] plays to win, it never chooses some branches
                                    allowing [Human] to win**, we want to
                                    **Mark these branches as -1 , as we want to Avoid future opportunities
                                    that MAY result in our loss, and;**

                            * **Upward Arrow** nodes on way up, pick the Max Value among its Child Nodes
                        * Note that if [Human] player makes a wiser choice at a higher tree level
                        it can avoid situation of [Computer] being able to play to win


            * **Multi-Player, Probabilistic Games**

        * Problem Solving and Game Playing
            * Strategy Games - i.e. checkers, isolation, othello, chess
            * Algorithms
                * **Reusable code, except modifying the code that
                generates all the next possible moves and the UI**,
                to create Computer Player for simple **Deterministic**
                games (where outcome of each move is known) such as
                Tic Tac Toe, or as complex as Chess
                * **Chance** games (i.e. backgammon)


        * Max Qty of Nodes Visited
            * Given
                * Isolation Game board: 5x5
                * Time to process all Upper Limit of Nodes:
                    ```
                    # Move 1 [Computer] - 25 possible places
                    # Move 2 [Human]    - 24 possible places
                    #                     23 ...

                    # i.e. 25 x 24 x 23 x 22 ... x 1
                           = 25!
                           = 10 ^ 25

                    # assume multi-core gigahertz processor able to do
                    # 10 ^ 9 operations/s it will take 10 ^ 16 seconds
                    # to search the entire game,
                    # where 10 ^ 16 sec
                      given 3600 sec/hr
                      given 24 hr/day
                      given 365 day/yr
                      = 317,097,920 years
                    ```
                * Time to process only up to Second Move:
                    ```
                    # 25 x 24 = 600
                    ```
                * Subsequent Moves from Third Move:
                    * Each subsequent move moves like a queen and less moves
                    * **Center** Position on board with Max Options to move on Third Move
                * **Branching Factor**
                    * Center has 16 possible positions
                    * Other Positions have <= 13 possible positions
                    * Max moves (depth of tree) in a game is 25
                    * Max possibilities from First and Second moves: 25 x 24 = 600
                    * 23 remaining moves after First and Second moves:
                        * <= 13 possible moves available (except center position)
                    * End Nodes Worst Case in game tree:
                        * 25 * 24 * 12 ^ 23 =  >10 ^ 27 estimate
                    ...
                    * Branching Factor of 12! factorial is far too much to
                     assume as being necessary for the last moves where not many
                     possibilities remain
                        * Fourth Last Move possibilities = <=3
                        * Third Last Move possibilities = <=2
                        * Second Last Move possibilities = 1
                        * Last Move possibilities = 0
                    * Instead assume this approximate is sufficient
                        ```
                        12 ^ 12 ~= 10 ^ 13
                        12 * 11 * ... * 3 * 2 * 1 = 5 * 10 ^ 8

                        25 * 24 * (12 ^ 13) * (5 * 10 ^ 8) ~= 3 * 10 ^ 23
                        (much better/faster than 10 ^ 27 since most branches will have less
                         than the max branching factor)
                        ```
                    * **Qty of Nodes in Game Tree that Minimax Algorithm must visit
                      to find Optimal solution is**:
                        * **b ^ d**
                            * Given grid: 5x5
                            * `b` (avg branch factor)
                            * `d` (depth of game tree)
                    * **Calculate Average Branching Factor**
                        * Option 1: **Brute Force**
                            * Try simple test first and add intelligence later
                            * Create game player than moves randomly and calculate
                              the avg branching factor
                              (use statics on each move: branches remaining, total games,
                               total plays, total branches, avg branching)
                              and we find the
                                * **Average Branching Factor**: 8
                                * **Estimated Max Nodes visited**: 8 ^ 25 ~= 10 ^ 22 (i.e. 1.2 million years for answer)
                        * Option 2: **Depth Limited Search** (Clever alternative to long end game search)
                            * Assume:
                                * Search Speed: 10^9 nodes * 2 sec  =  2 * 10^9 nodes
                                * Solve:    8^x              <  2 * 10^9
                                            log base 8  8^x  <  log base 8  2*10^9

                                    ```
                                    Math Formula:
                                    log base a  x  =  log base b  x  /  log base b  a
                                    ```

                                * So...:   x  <  log base 10  2 * 10^9 / log base 10  8
                                           x  <  10.3

                                    * Assuming **Branching Factor of 8** is good, to be on the safe side
                                      we should only go to **Max Depth of 9**
                                      (only go as deep as think need to meet deadline)

                                    * At **Depth of 9** we want an **Evaluation Function
                                    of a Node** to evaluate goodness of node at depth of 9
                                    based on how much we expect it to lead to Win for
                                    [Computer] player

                                        * Note: Only way for [Computer] player to Win is for it to have
                                        moves left at end of game, so maybe [Computer] **shouldn't
                                        maximize the number of moves it has early on**!

                                        * **Evaluation Function (aka "Number of #my_moves for convenience for Player 0")**
                                            * **Note** On each game tree branch
                                                * Min Branch (third last) - Mark min value of all below Max Branches
                                                * Max Branch (second last) - Mark max value of leaves below it
                                                * Leafs - Mark the maximum about of moves available to [Computer]
                                            * Arguments: Each game board generated level 9 of Minimax game tree
                                            * Returns: Number to compare that level 9 node with all other
                                            nodes at level 9. A higher number is returned is representative of how
                                            good the board is for the [Computer] player
                                            * Task: Counting number of moves the [Computer] has available at
                                            a given node, allows [Computer] to select branches from Minimax tree
                                            that lead to spaces where [Computer] has the most options
                                            * Also, use the **Evaluation Function** to test our **assumption**
                                            that if [Computer] makes a move to any middle position they will always lose.
                                            Try ONLY applying **Evaluation Function** down to Level 2 of game tree,
                                            to see that completely different nodes get higher scores, (the ones that
                                            would guarantee a loss!! Maybe they vary widely because critical decision being
                                            made at that level.
                                                * Demonstrates better to search to at least level 3
                                                * Check which branch the **Minimax Algorithm** recommends at
                                                each level of the search
                                                    * Use #my_moves Evaluation Function on Isolation Game with
                                                    Max score 5, Min score 0. If find branch where
                                                    [Computer] loses, give it score -1, or if find branch
                                                    where [Computer] wins, give it score 100..

                                                    * **Quiesence Search** is the state we reach if we continue to Level 3,
                                                    where the recommended branches no longer
                                                    change much so know we have gone far enough... and if continue
                                                    to Level 5, we just learn what we already know and values
                                                    become -1's and +1's
                                                        * Use this approach of determining deepest level to go to
                                                    if get consistent results
                                                        * Use this approach at Start and End of game when gives better results



                                                * **Depth-First Iterative Deepening (DFID)**
                                                    * References:
                                                        * AI Textbook: 3.4.5 of Russel, Norvig
                                                        * UBC Slides: https://www.cs.ubc.ca/~hutter/teaching/cpsc322/2-Search6-final.pdf
                                                        * Video showing how DFID vs DFS http://movingai.com/dfid.html

                                                    * Note: End Game is best critical time to apply DFID to see what will happen
                                                    (i.e. in Isolation game)
                                                    * Note: Using DFID means that our [Computer] player will always have an ANSWER READY
                                                    incase it runs out of time (whe limited time PER MOVE), and using it we can search as far as possible within its
                                                    time constraints. In other games where limited time PER GAME (i.e. speed chess)
                                                    so we would want [Computer] to search to different depths depending on point in the game
                                                    * **Strategy developed on how deep to search at different points in the game**
                                                        * **Strategy 1:**
                                                            * Beginning of game: **Standard Initial Moves**
                                                            * Middle of game: **Deeper Search (more time allocated) for Moves**
                                                            * End of game: **Less Time searching (as far as possible in remaining time) for Moves**
                                                            (relying on reduction and branching factor)
                                                        * **Strategy 2:**
                                                            * Conservative amount of time dedicated per Move
                                                            * Use DFID and Quiescent Search to determine few moves want to spend extra time on
                                                        * RISKS:
                                                            **Horizon Effect**
                                                                * Highlighting to [Computer] player critical
                                                                situations when game will be won/lost on next move
                                                                (but where [Computer] unable to search far enough
                                                                into future to discover problem exists)

                                                                * i.e. If a player is blocked on a side of
                                                                a Partition where they have less moves than if they
                                                                chose to move to the other side of the Partition
                                                                (in the board where Partition is created by previously
                                                                occupied positions)

                                                                * **Potential Solution** Add a process in your existing #my_move
                                                                **Simple Evaluation Function** to make it a
                                                                **Complex Evaluation Function** by additionally checking if a
                                                                **Partition** is being formed by the next move, and if so, count
                                                                the number of moves left to each player
                                                                (BUT this causes our Evaluation Function to take exponentially longer)

                                                                * **Solution**
                                                                    * **Strategy** Depending on game time available, carefully think
                                                                    how efficiently Evaluation Function may be implemented depending on
                                                                    strategy chosen, and whether it captures desired behaviour or not,
                                                                    either use. But BEFORE the **Evaluation Function**
                                                                    use **Alpha-Beta Pruning** (to improve efficiency of game tree search):

                                                                        * **Alpha-Beta Pruning Algorithm**
                                                                            * Dfn: Pruning technique to allow ignore whole sections
                                                                            of game tree but with same answer as with **Minimax Algorithm**
                                                                            so is MORE EFFICIENT, saving a lot of time.
                                                                            Where
                                                                            * **Minimax Algorithm** runs in a timeframe of `b ^ d`
                                                                            * **Minimax with Alpha-Beta Pruning (first)** runs in a timeframe of `b ^ d/2`
                                                                            (if nodes ordered optimally with best moves first), and down to timeframe of
                                                                            `b ^ d*3/4` (with random move ordering)
                                                                                * Firstly, When performing the **Minimax Algorithm** from left to right,
                                                                                after propogating the highest value up to the far left most bottom
                                                                                Max Node (Level -2) from that branch's two nodes (Level -1, aka leaf),
                                                                                (i.e. a value of 2) we know that the upward Min Node's (Level -3) subtree
                                                                                value will be <= 2... (since Opponent X will choose branch that minimises
                                                                                the value), so for each remaining Level -2 nodes, as soon as we find
                                                                                 they have a leave Level -1 node of value 2, we can ignore the rest of
                                                                                 its leaves and move on to next Level -2 node (i.e. if second
                                                                                  Level -2 Max Node has 3x leaf nodes, and we find first one is 2,
                                                                                  we can ignore the other two as they will never be selected), saving time!!
                                                                                  And based on this we know that at the highest Top node Level -4,
                                                                                  we will get a value of >= 2
                                                                                * Then, assuming Level -4 is the Top of the game tree.. when we
                                                                                 check the second from the left Level -3 subtree's ([Computer] turn)
                                                                                 Level -1's leaf nodes,
                                                                                 as soon as we find a value of 2 we know that is a Winning leaf, with its
                                                                                 upward Level -3 subtrees Min value being <= 2, so we can ignore the rest of
                                                                                 the branches below that subtree
                                                                                * Then, for third from the left Level -3 subtree's Level -1's leaf nodes,
                                                                                 as soon as we find a leaf with value of 2, we can ignore the remaining Level -2's
                                                                                 and associated leafs in that subtree, since we have already found a Max (Winning)
                                                                                 move in that subtree
                                                                                * Note: The above assumes our goal is to play the game, and that
                                                                                we choose from left to right using Minimax Algorithm, but not keep a list of all
                                                                                the equally good moves on a given level

                                                                                * **Reordering leaf nodes by only swapping siblings of a given parent**
                                                                                to increase amount pruned
                                                                                    * i.e. Since evaluate left to right, in right
                                                                                    subtree swap the Level -2 Max nodes around
                                                                                    so smallest is on left instead of on right, as Level -3 is Min, and may
                                                                                    only need to evaluate the left if we find Level -3 Min is less than
                                                                                    first subtree


                                                                        * **"Simple" Evaluation Function** and DEEP SEARCH, OR
                                                                        * **"Complex" Evaluation Function** and SHALLOW SEARCH
                                                                        to catch dangerous situations like the **Horizon Effect**, such as
                                                                        extensions of the **#my_move** **Simple Evaluation Function**
                                                                        that are correlated with our chances of winning including
                                                                        (try lots of Variants of Complex Evaluation Functions to see
                                                                        which ones are the best):
                                                                            * (**#my_moves** - **#opponent_moves**)
                                                                                * Note: Useful for Simple Isolation games
                                                                                * Favours moves where [Computer] has most options
                                                                                * Penalises moves where opponent has most options
                                                                                * Note: Possible to **weight the formula components** for
                                                                                more or less aggressive game play.
                                                                                    * i.e. to cause [Computer] to chase after Opponent, and
                                                                                    to make it so the **Winning Move has the highest Evaluation
                                                                                    Function result**
                                                                                        * (**#my_moves** - 2 * **#opponent_moves**)


                                                    * Init Goal: Want [Computer] to respond within < 2 sec per move
                                                    * Previously used Max Depth Calc: Calculated Max Depth we thought could search to
                                                    within that timeframe
                                                    * Simpler Iterative Deepening Approach:
                                                        * Search to Depth Level 1, get answer for best move,
                                                        Store answer to use incase run out of time
                                                        (L1 has 1 tree node, L2 has 3 tree nodes,
                                                        L1 has 1 iterative deepening nodes total,
                                                        L2 has 4 iterative deepending nodes total)
                                                        * Repeat process but search to Depth Level 2
                                                        * Repeat process until run out of time (returning best move
                                                        from level we get up to)
                                                    * Note: **number of Tree Nodes for each level** is the total number of
                                                    nodes visited when exploring the tree till that depth (not just
                                                    the nodes on that level)
                                                    * Note: **number of Iterative Deepening Nodes are simply the cumulative
                                                    sum at each level** (always less than double the number of tree nodes)
                                                    * Note:
                                                        ```
                                                            b = 2                    (Branching Factor of 2 given)
                                                            n = 2^(d+1) - 1          (Tree Nodes total)

                                                            b = 3                    (Branching Factor of 3 given)
                                                            n = (3^(d+1) - 1) / 2    (Tree Nodes total)

                                                            b = k                       (Branching Factor of k given)
                                                            n = (k^(d+1) - 1) / (k - 1) (Tree Nodes total)
                                                        ```
                                                    * Note: Iterative Deepening of researching the tree
                                                    does not waste too much time as
                                                    amount of time taken is dominated by last level searched
                                                    and is a small multiplier compared to
                                                    the DFS approach of checking every node from bottom leaf left to right
                                                    (exponential)
                                                    * Note: With Branching Factor of 2, DFID expands less than 2x
                                                    the qty of nodes a depth limited search would have done at the same level
                                                    * Note: With Branching Factor > 2 (i.e. most games) then the redundancy
                                                    caused by iterative deepening is even less

        * AI Definitions
            * AI NP (Hard) Problem Types
                * Non-Exponential
                * Exponential in Time OR
                * Exponential in Space OR
                * Exponential in Both Time AND Space
            * i.e. "AI is the studying of finding clever hacks to exponential problems"
                * When problem finally solved or computer finally fast enough to solve it,
                the world no longer thinks of the problem as belonging to AI
            * i.e. "AI consists of all the NP hard problems that have not yet been solved"
            * i.e. "AI involves working on everyday problems that improve people's lives"
            * AI Systems
                * System that helps consumers choose plane flights
                * System that determines when to deploy airbag in a car

        * Lesson 6.16:
            * Solving 5x5 Isolation Game
                * Searching to endgame in reasonable timeframe:
                    * **Minimax Algorithm with Alpha-Beta Pruning**
                      (reduces search space size of possible game states from b^d to b^(d/2)
                       i.e. from approx 8^25 to 8^12)
                    * **Symmetry Analysis** of board state that is defined as series of ordered moves
                        *
                            http://stackoverflow.com/questions/9082829/creating-2d-coordinates-map-in-python
                            http://stackoverflow.com/questions/32334322/find-adjacent-elements-in-a-2d-numpy-grid
                            http://stackoverflow.com/questions/6313308/get-all-the-diagonals-in-a-matrix-list-of-lists-in-python
                            http://stackoverflow.com/questions/2373306/pythonic-and-efficient-way-of-finding-adjacent-cells-in-grid

                        * Best at **Beginning of Game** and **Up to Level 3 of the Search Tree**
                          (beyond Level 3, symmetry is rare, and effort to check for it wasn't worthwhile)
                          (when Branching Factor is High,
                          i.e. Player 1 has 25 possible moves, but only 6 Unique moves,
                           since can use **Symmetry** about horiz, vert, diagonal axis, and
                           the center move)
                            * **Heuristic of Equivalent Moves by Rotating Board State 90 degrees**
                              (realising some moves equivalent)
                                * Reuse Game Tree known for Move (0,0) as is same as (4,0), (4,4), (0,4)
                                * Do not have to search the game tree further for these non-unique moves
                    * **Partitions** (know the game outcome as soon as get partition,
                    as separates both players completely, so player with longest path left Wins,
                    so avoid having to search to end of game tree)
                        * Best at **Beginning of Game** to create Partition
                          so your player is on side with most remaining moves
                    * **Avoid being Player 2**
                        * Note: Assuming both players play Optimally,
                          Player 2 always wins on 5x5 Isolation game
                    * **If Player 1, always first Move to Center square,
                        and then if possible use Reflection on all subsequent moves to win**
                        * **Reflection Phenomenon** is always copying the opponents move
                        180 degrees on other side of board, on every move they make
                            i.e.
                                Player 1 Move 1: Center
                                Player 2 Move 1: Diagonal
                                Player 1 Move 2: Opposite Diagonal, etc
                    * **If Player 2, detect if Player 1 is using Reflection Phenomonon and if
                      so, move to a position that Player 1 cannot reflect** (there are 8 such moves
                      of the 24 available to Player 2)
                    * **If Player 2, create Book of Opening Moves and Hints,
                    such as:**
                        * **If Player 1's first move is to Center Square, then Player 2's first
                        move afterward should be an Unreflectable position**
                        * **If Player 1's first move is NOT Center Square, then Player 2's first
                        move afterward should be the Center square**
                    * **After first moves,:
                        * **Load and Search our order
                          Order Book of Opening Moves and Hints efficiently, comprising**:
                            * Equivalent Moves
                            * Hash Tables
                        * Implement in order:
                            * **Minimax**
                            * **Iterative Deepening**
                            * **Alpha-Beta Pruning**
                            * **Evaluation Function**

        * Lesson 6.17 - 6.18
            * Multiplayer Games + MAXN (for any number of players)
                * 3-Player Isolation Game
                    * Last player able still able to move wins
                    * Pairs of players may form alliances
                    * Do Not use Minimax Algorithm in multi-player games
                    * Evaluate game board Leafs from perspective of
                    each player and propagate values (as triplets)
                    up the tree, i.e.
                        * Level 3 Max triplet from Player 3's perspective
                          (i.e. propagate up triplet leaf with largest Player 3 value
                        * Level 2 Max triplet from Player 2's "
                        * Level 1 Max triplet from Player 1's "
                        * Level 0 (Top) receives propagation from Level 1

        * Lesson 6.19
            * Multiplayer Games + Alpha-Beta Pruning
                * Lesson 6.21
                    * Reading:
                        * References: [Korf, 1991, Multi-player alpha-beta pruning](http://www.cc.gatech.edu/~thad/6601-gradAI-fall2015/Korf_Multi-player-Alpha-beta-Pruning.pdf)
                            * Why might alphabeta pruning only work well in the two player case?
                                *
                            * How does the size of the search tree change with more than two players?
                        * Questions Asked in Forum https://discussions.udacity.com/t/lesson-6-part-21-multi-player-alpha-beta-pruning-paper-reading-by-korf-figure-7-and-section-3-5/226209
                * A-B Pruning works if:
                    * Some players'
                      **Evaluation Functions** have an Upper Bound
                    * All players'
                    **Evaluation Functions** have a Lower Bound
                * **Evaluation Function** for number of #my_moves has:
                    * Lower Bound of 0
                    * Upper Bound - calculate using this **Evaluation Function**:
                        * **Evaluate Function** should sum of total number of
                        moves remaining for each player, which should not exceed
                        remaining number of empty squares on isolation board
                        (if 3x players, divide number of empty squares by 3 to get
                        moves remaining for each player).
                        Note: The Upper Bound would change with each game tree
                        Level.
                        * Simplify calculation that allows both
                          **Immediate Pruning** and **Shallow Pruning**,
                          BUT NOT **Deep Pruning** like before, and use
                          **3-Player MAX-MAX-MAX Pruning** (see Lesson 6.20):
                            * SUM of players scores (within a triplet group) must be Max of 10, i.e.
                                 * Level 2 Max triplet from Player 2's  perspective
                                 (i.e. if a Leaf has Player 2's [middle] value of 10 value
                                 where Player 1 and Player 3 value is 0 since Max SUM of all three is 10,
                                 then propagate up triplet Leaf with largest Player 2 value,
                                 and prune remaining leaves on that branch)
                                 * Level 0 (Top) receives ranges based on triplets propagated up,
                                  which gets refined on outcome of propagating up subsequent subtrees, i.e.
                                     (>=3, <=7, <=7)
                                 * When doing subsequent subtrees, apply the range
                                 to Level 1 (i.e. (>=3, <=7, <=7) ), and if the subsequent branches
                                 have player values already within those ranges but
                                 less than the player values we have propagated up
                                 in previous subtrees to Level 1, then Prune them

        * Lesson 6.22 Probabilistic Games
            * Stochastic Games (i.e. Backgammon)
                * Note: i.e. Moves limited on each turn, based on outcome of rolling two dice
                    (do not know value of each dice ahead of time, we assume we cannot
                    do a game tree for it, but we can)
                * Game Tree
                    * **Expectimax Algorithm** to make  Best Choice decisions on Moves
                        * Only allowed to Prune when have Known Bounds on the values that will be returned
                        by the Expectimax Function
                        * If player is going to lose anyway, unless the Opposition makes a
                         bad choice or is unlucky, then still use **Expectimax Algorithm**
                         to increase likelihood of winning incase Opposition makes
                         bad choice or is unlucky, BUT we would need to search all the way
                         to endgame
                        * When evaluating leaves, process highest leaf value first with highest probability
                        (change order) for the left-most branch

                * Sloppy Isolation (Variant of Isolation Game that is a Probabilistic Game)
                    * Add proability nodes between Levels to illustrate probability of
                    making the move vs a different move with probability of between 0 and 1
                    * Use #my_moves **Evaluation Function** at leaf nodes, if make move
                    at leaf node, calculate how many moves will still be available for the player
                    to move to after the oppositions next move, and for each leaf multiple this value
                    by the probability of that move occurring compared to others in the same branch,
                    then sum those values for each leaf node
                    * Elimate evaluating nodes all together if return is higher when looking for
                    Min Level

    * Project 2:
        * Goal: Program player that beats opponent consistently
        * Design different Heuristics to perform the Adversarial Search
        * Analyse and compare the performance of each Heuristic

        * Reference Projects by others:
            *
                ```
                https://github.com/morganel/AIND-Isolation-1
                https://github.com/bdiesel/AIND-Isolation
                https://github.com/ziyanfeng/AIND-Isolation
                https://github.com/davidventuri/udacity-aind/tree/master/isolation
                https://github.com/mohankarthik/AIND-P2-Isolation
                https://github.com/mlgill/AIND-Isolation
                https://github.com/morganel/AIND-Isolation-1
                ```

        * Reference Game Simulation https://deepmind.com/research/alphago/alphago-games-english/
        * Summary of AlphaGo, excellent https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/


    * Lesson 7 - Search:
        * **AI dev** dfn: Figure out what to do when don't know what to do
            * Plan sequence of steps to figure out what to do even when do not know
            what first step should be until we solve the search problem
            * Note: Experiment and keep measuring so you can improve.
            Pay attention to people by keeping them happy and productive
            * **A\* Algorithm**
        * **non-AI dev** dfn: Write instructions to tell computer what to do when you do know what to do
        * Challenge 1:
            * Tri-City Search Problem
                * **Problem** is visit 3x places on bicycle, minimising travel distance, and willing to
                 go down wrong way down one-way streets, with minimal runtime, and minimal overhead
                * **Step 0**
                    * Consider Space and Time advantages/disadvantages of each algorithm
                * **Step 1**
                    * Option 1: Simultaneously (trick) start Search from A, B, and C
                    using **A\* Algorithm** (aka Tri-Directional A\* Search),
                    by growing them out until first two connect, then continuing
                    until third location connects somewhere with first two
                    (avoids repeated node visits)
                        * **Tri-Directional A\* Search** uses an Admissible **Heuristic**
                        function that accounts for **two potential goals at the same time**
                        otherwise would just be an uninformed Tri-Directional Search
                    * Option 2: DFS - not viable as may cross country multiple times
                    just for first branch
                    * Option 3: BFS (UCS) - works, but 3x searches required,
                        * Sequentially Search from: A to B, B to C, and C to A,
                        then determine Shortest two of the three paths,
                        and set the polling station between those two paths as the
                        next to visit and start at either of the other two cities
                        * Issue: Would explore more streets intersections / nodes
                        repeatedly in the 3x searches more than necessary
                        (duplicates), and search range is further than necessary
                    * Option 4: Bi-Directional Search
                        * Instead of each of 3x searches requiring **b^d nodes**
                        it would be done in **(b^d)/2 nodes**, but would still revisit
                        nodes in triangle b/w the cities

        * Challenge 2:
            * Rubik's Cube
                * **Problem** Goal of all same colours on same side, cube levels twistable on
                vertical or horizontal. Design search algorithm that guarantees least qty moves to
                finish from any starting state, where each 1/4 (90 degree) turn is a move
                * **Solution** Iterative Deepening with A\* Search Algorithm (IDA*), which
                looks for increasingly longer solutions in series of iterations, and using a special
                admissable Heuristic (search technique) described here [Korf, 1997. Finding Optimal Solutions to Rubik's Cube Using Pattern Databases.](https://www.cs.princeton.edu/courses/archive/fall06/cos402/papers/korfrubik.pdf),
                that uses a lower-bound heuristic to prune branches when their estimated
                length exceeds the current iteration bound, and a heuristic function
                based on large memory-based lookup tables
                or "pattern databases" (storing exact qty of moves req'd to solve sub-goals
                such as individual movable cubies) and where the effectiveness of the
                admissible heuristic function is characterised by is **expected value** (see paper
                for details),
                searching to median depth of 18 for random configs attempted.
                Proved that max quarter turns required to solve cube is 26  [God's Number is 26 in the Quarter-Turn Metric](http://www.cube20.org/qtm/)
                using 29 CPU years of idle computer time of super computer
                    * Note: Overcomes issue since search space is large (i.e. nodes at depth of 18
                    in search tree is Quintillion (i.e. xx,xxx,xxx,xxx,xxx,xxx,xxx) so is Intractable
        * **"Problem Solving" AI-Agents**
            * i.e. theory and tech of building AI-agents to plan ahead to solve problems,
            where problem complexity is due to many problem states
            * **Conditions that must be True in order to successfully
            search for a plan that solves a problem and be guaranteed of working**
                * Domain must be **Full Observable**
                (i.e. must be able to see initial state we start with)
                * Domain must be **Known**
                (i.e. must know the actions that are available to us)
                * Domain must be **Discrete**
                (i.e. must be finite qty of actions to choose from)
                * Domain must be **Deterministic**
                (i.e. must know the result of taking an action)
                * Domain must be **Static**
                (i.e. must be nothing else in world that can change the world
                excpet our own actions
            * **State Spaces** of many Types may be dealt with using Problem-Solving through Search,
            such as:
                * 2D
                    * Route finding problems
                    * Sliding blocks puzzle called "15 Puzzle"
                        * Note: **requires a Heuristic function to be applied in order for the
                        search algorithm to work**
                        * Starting State is mixed up positions
                        * Approach - Slide tiles around until reach goal
                        * Goal - Tiles in certain configuration in order left-to-right
                        and top-to-bottom
                            * i.e.
                            ```
                             1  2  3  4
                             5  6  7  8
                             9 10 11 12
                            13 14 15
                            ```

                        * **Heuristic Usage** Solution Approach
                            * Heuristic h1 = qty of misplaced blocks
                                * Admissible (never overestimates, since each time
                                in wrong position must be moved at least once to get
                                to right position)
                            * Heuristic h2 = sum of distances that each block would
                            have to move to get to right position
                                * Admissible (since each tile in wrong position
                                can be moved closer to correct position no faster
                                than one space per move)
                            * Heuristic h3 = always exact cost at every node
                            (so naturally expands the least number of nodes)
                            * Heuristic h4 = always 0 (so naturally
                            expands the most number of nodes of all possible heuristics)
                            * Note: h2 >= h1
                                * So with the exception of breaking ties, an
                                A\* Search using h2 always expands fewer paths
                                than one using h1
                            * Note: Heuristic that is greater than or equal to another
                            one gets use closer to the perfect heuristic and expands
                            at least the same qty of nodes
                        * **Deriving Candidate Heuristics Automatically
                        (aka Generating a Relaxed Problem)** (by manipulating a formal
                        description of the problem where many constraints, such as harder to move
                        tiles around, and made easier by relaxing one or more of the constraints)
                            * Note: By **relaxing the problem**, it is
                            as though new operators are being added that traverse the state in new ways,
                            making the problem **easier** to solve (never overestimates, and is admissible)
                            * Program that is:
                                * Given Description of a Problem (i.e. sliding blocks puzzle)
                                ```
                                Line 1 - A block can move from any tile A to any tile B
                                Line 2 - if (A is adjacent to B)
                                Line 3 - and if (B is blank tile
                                ```
                            * Loosen the restriction by:
                                * Crossing out Line 3, results in **Heuristic h2**
                                (since a block can move anywhere to an adjacent state)
                                ```
                                Line 1 - A block can move from any tile A to any tile B
                                Line 2 - if (A is adjacent to B)
                                ```
                            * Loosen the restriction again further by:
                                * Crossing out Line 2, results in **Heuristic h1**
                                (where a block can move anywhere, regardless of any condition)
                                ```
                                Line 1 - A block can move from any tile A to any tile B
                                ```
                        * **Generating Another Heuristic**
                            * Guaranteed Admissible (as long as h1 and h2 Admissible) and
                            never overestimates, and is guaranteed better as gets closer to true value
                            ```
                            h = max (h1, h2)
                            ```
                            * Note: Problem with combining heuristics is that there is cost to compute


                * 3D
                    * i.e. Abstract properties other than x-y position on a plane
                    * Robot vacuum cleaner world state space
                        * In either of only 2x possible positions
                            * Each position may or may not have dirt in it
                            (additional property to deal with)
                        * States Space required: 2x2x2 = **8** states
                            * 2x physical states (State A or B)
                                * 2x dirty states for State A (dirty A-dirty or not A-not-dirty)
                                * 2x dirty states for State B (dirty B-dirty or not B-not-dirty)
                        * Actions connect between each State
                        * Example 1 (Simple):
                            * Action of Move Right - Changes to State with position to the right that is dirty
                            * Action of Sucking - Change to State with same position but no longer dirty
                        * Example 2 (More Complex):
                            * Given possible Robot positions 10 OFF
                                * Given Robot possible positions
                                    * 10 possible positions
                                * Given Power Switch in one of 3 possible positions
                                    * On
                                    * Off
                                    * Sleep
                                * Given Robot with Dirt Sensing Camera either
                                (2 possible positions):
                                    * On
                                    * Off
                                * Given Robot with vacuum Brush Heights 5 OFF
                                (5 possible positions):
                                    * 1-5
                                * Given Dirty or not
                                (2 possible states)
                                    * Dirty or Not Dirty

                            * State Space has qty of states of:
                                * Cross product of all variables (since each independent
                                and any combination may occur)
                                * = 3 * 2 * 5 * 2^10 * 10 = 307,200 states
            * **Paths in the State Space**
                * **Linked List** of Nodes implement the Paths within a computer algorithm,
                where a Node is a Data Structure that has four Fields.
                    * State Field - indicates the state at end of the Path (i.e. B)
                    * Action - action take to get there (i.e. AB)
                    * Cost - total cost and parent is pointer to another node (i.e. 234)
                    * Parent - pointer to another node state (i.e. A), or null pointer
                    if no parent
                * Note:
                    * Path - abstract idea
                    * Node - same thing as Path, but terminology for
                    representation in computer memory
            * **Data Structures that deal with Nodes/Paths and their implementation**
                * Frontier
                    * Operations
                        * Remove best item
                        * Add new items
                        * Membership test (i.e. is item in frontier?)
                    * Implementation
                        * **Priority Queue** (knows how to keep track of best
                        items in proper order)
                    * Representation
                        * Set (i.e. built from Hash Table or Tree)
                    * Build
                        * Hash Table
                        * Tree
                * Explored List
                    * Operations
                        * Add new members
                        * Check for membership
                    * Implementation
                        *
                    * Representation
                        * Single Set
                    * Build
                        * Hash Table
                        * Tree
            * Example 1: Navigation WITHOUT Fog Problem
                * Initially many choices
                * Complexity in **Sequence of Actions** (i.e. of picking right choice
                of turn at each intersection)
            * Example 2: Navigating WITH Fog Problem
                * Complexity in **Partial Observability** (i.e. if unable to see through fog
                where possible paths are, actions themselves are unknown, and
                can't see result of actions)
            * **Example A: Route-Finding 2D Problem (Partial Observability)**
                * Given
                    * Start location (i.e. Arad)
                    * Destination location (i.e. Bucharest). Note: Corner of map
                    * Partial map not showing the Destination
                * Problem:
                    * Find route from Start to Destination location
                    * Actions executable by AI-agent is driving from city-to-city along roads
                    shown on a map
                * Question:
                    * Is there a solution the AI-agent can return?
                * Answer:
                    * No, not if Start or Destination are not on the map
            * **Example B: Route-Finding Problem (Full Observability)**
                * Given
                    * Same as above but Full map showing Destination
                * Problem
                    * Same as above
                * Question
                    * Same as above
                * Answer
                    * Yes, many possible Sequences of Actions (routes) chained together that
                    each achieve the goal
            * **Defining a Problem**
                * Break down into components
                    * **Initial State** (of AI-agent) - s0  (i.e. being in Arad)
                    * **Actions** Function takes state input argument and returns
                    set of possible actions executable when agent in the state
                    (i.e. sometimes same actions available in multiple states)
                    (i.e. sometimes actions are dependent on the state) (i.e. possible to go to
                     neighboring cities but not any city in one move)
                        * myActionFunc(s) -> {a1, a2, a3}
                    * **Result** function takes state and action as input argument
                    and returns new state as output
                        * myResultFunc(s, a) -> s'
                        * i.e.
                            * State - Initial AI-agent state - s0 (in Arad)
                            * Action - Go via route E671 to Timisora
                            * Result - New State where AI-agent in Timisora
                            (of applying Action in given State)
                    * **GoalTest** function takes state and returns Boolean whether state
                    is goal or not (i.e. only True in final destination)
                    * **Path Cost** function takes a path, sequence of transitions
                    (from performing Actions to move between States), and returns
                    Cost of the path. Note: Usually just additive (i.e. Path Cost
                    just sum of cost of individual Action steps) so implement the
                    Path Cost function in terms of a **Step Cost** function (i.e Path cost
                    is just the sum of the Step Costs along a path)
                    * **Step Cost** function takes a State, Action, and the
                    Resulting State from that Action, and returns the **Action Cost** as a
                    number (n)
                        * i.e. in route finding example this cost may by qty of kms
                        travelled or minutes taken to get to destination)
            * **Apply to Route Finding example the Definition of a Problem**
                * Given **Initial State** of Arad
                * Given **Goal State** of Bucharest
                * Given possible **Actions** of roads to follow when in specific city
                * **Sequences of Actions** are built from paths we follow
                * Note: **Path Length** of 0 when just in **Initial State**
                * Note: **Path Length** with "X" **Sequences of Actions** from
                Initial State has **Path Length** of "X"
                * Note: **State Space** is the set of all states
                * Note: **Actions** are used to navigate the State Space
                * Note: **Step Cost** of going between two adjacent States are shown
                * Note: **Path Cost** for a path is sum of Step Costs along that path
                * Note: At each Point we separate **State** into 3x different components:
                    * **Frontier** consists of the States at the end of the Paths
                    that have been explored
                    * **Explored** region States before the end of the Paths that have been explored
                    * **Unexplored** region States after the end of the Paths that have been explored
            * **Define a Function for Solving Problems** called **Tree.Search**
                * Note: Tree.Search represents a family of algorithms/functions, not a single algorithm,
                which superimposes a search tree over the state space
                * Note: It initialises frontier to be path consisting of only Initial State,
                then a loop checks if still anything left in Frontier, otherwise fails
                with no solution. If something left in Frontier we make a choice and use the
                Tree.Search's family of functions to choose one of the paths in the Frontier,
                and then remove that path from the Frontier. If we find the State at the
                end of the Path that is a Goal State we return success, Else we
                perform **Expanding that Path** by iteratively inspecting all possible Actions
                expanding from that State and add to the Path the Actions and the result of that
                State to get a New Path (which has the Old Path, the Action, and result of the Action).
                All New Paths are added to the Frontier
            * **Tree.Search Family of Algorithms - Commonalities and Differences**
                * Commonalities:
                    * All algorithms look at Frontier, popping items to check against Goal Test
                * Differences between the algorithms:
                    * The "choice" of which path to look at first (i.e. `remove.choice` of
                    how you expand the next item of the Frontier)
            * Search Algorithms
                * **Breadth-First Search (BFS) Algorithm (aka Shortest steps first)**
                    * Dfn: i.e. Searches by expanding first the shortest possible path steps,
                    since always choose from the Frontier one of the paths that hasn't been
                    considered yet, which is shortest possible
                    * Optimal and Complete (if goal at finite depth)
                * **Cheapest-First (Uniform Cost) Search (UCS) Algorithm (aka Cheapest cost first) (aka Dijkstra's algorithm)**
                    * Dfn: i.e. Searches by expanding first for path with cheapest possible total cost first
                    * Optimal and Complete (if goal has finite cost)
                * **Depth-First Search** (Opposite of Breadth-First Search)
                    * Dfn: i.e. Searches by expanding first the longest path,
                    with the most links in it
                    * NOT Optimal, and NOT Complete (since if infinite path, DFS would keep following it
                    and never get to path consisting of the goal)
                    * DFS only uses Storage Space of n nodes (instead of 2^n nodes in BFS and CFS)
                    so saves a huge amount of space. But if also track the "Expored Set" we do not
                    save as much.
                * **Greedy Best-First (GBF) Search Algorithm (UCS focussing on Estimate)**
                    * Dfn: i.e. Searches by expanding first the path that's closest to the goal
                    according to the **Estimate** (search leads us directly toward the goal but searching in ellipses
                    toward the goal, instead of circles searching in all directions even away from the goal.
                    * Note: If **obstacles/barrier** in the way, may lead us to the
                    goal in a longer path than necessary if we continue toward the goal without back-tracking
                    back to states farther from the goal and exploring in the opposite direction around
                    the barrier)
                    * Similar to **Uniform Cost Search** by expanding in terms of Contours,
                    like on a topological map, first expanding
                    to a certain step distance, then to a farther distance, etc, from the initial state
                    until at some point we reach the goal state, but note that the search
                    was not directed toward the goal, but instead was expanding out everywhere in the space
                    and we should expect to explore Half the state space on average to find the goal,
                    so when the state space is large, we won't get to goal fast enough).
                    To **find the goal faster than with UCS, we add more Knowledge to the search**
                    by using an **Estimate of the Distance from the Start State to the Goal State**.
                    In Route finding problem we can move in any direction (up, down, left, right), but for
                    the **Estimate** we use the straight line distance between a State and Goal,
                    and use that **Estimate** to find way to goal fastest.
                * **A\* Algorithm (aka Best estimated total path cost first)
                (combines UCS and GFS)**
                    * Dfn: i.e. Searches by always expanding path with **minimum** value of the
                    Heuristic function f (defined as sum of g and h components) so result is a search
                    strategy that is the best possible (i.e. it finds shortest length path
                    whilst expanding minimum qty of paths possible),
                    where g of a path equals path cost,
                    where h of a path equals h value of the state (final state of the path),
                    which equals the estimated distance to the goal
                        * Heuristic Function (generated by relaxing the problem definition)
                        ```
                        f = g + h
                        g(path) = path cost (sum of path costs from initial state to current state)
                        h(path) = h(s) = estimated straight line distance to goal (from current state)
                        ```
                    * Note: h is a Heuristic function finds the lowest cost path since
                    when A\* Search ends it returns path P, with estimated cost C,
                    where C is also the actual cost, since the h component is 0 at the goal state
                    (so path cost is total cost, as estimated by the h function).
                        * Path P must have a cost less than true cost of other paths on Frontier.
                        * Paths beyond the Frontier must have cost greater than Path P's cost, since step
                        cost is always 0 or more
                        * Note: All paths on Frontier have estimated cost > C, since Frontier
                        is explored in cheapest-first order
                    * Note: Minimising g helps keep path short
                    * Note: Minimising h helps keep focused toward finding goal
                    * **Conditions under which A\* Algorithm will find the lowest cost
                    path to goal**: A\* will always find the lowest cost path to the goal dependent
                    on if the heuristic estimate function h for a state is less than the
                    true cost of the path to the goal through that state:
                        * h function want never to overestimates the distance to the goal
                        * h is **Optimistic**
                        * h is **Admissible** (since admissible to use it to find lowest cost path)
                    * Goal test works by taking paths off the Frontier, not putting paths on
                    the Frontier
                    * Additional Heuristic
                        * Use additional heuristic of the Straight Line distance between
                        a State and a Goal, for each State
                    * Combines **Greedy Best-First Search** of exploring a smaller number of nodes
                    directed toward the goal state, and **Uniform Cost Search** that is guaranteed
                    to find the shortest path to the goal state
                    * Example
                        * Given a found path through state space to State X, and trying
                        to give value to the path. The measure f is sum of g (path cost so far)
                        and h (estimated distance remaining for path to get to goal)
                * Note: Resolve ties in left to right order
                * **Optimality Property of Algorithms** (i.e. guaranteed to find the shortest path, cheapest total cost
                path, and longest path respectively)
                    * Yes, Optimal - Breadth-First Search
                    * Yes, Optimal (assuming individual step costs are non-negative) - Cheapest-First Search
                    * Not Optimal (if more than one goal state) - Depth-First Search
                * **Storage Requirements Property of Algorithms**
                    * Given a state space of "very large" Binary Tree (each level down
                    the nodes double)
                        * BFS "Frontier" requires at Level n a Storage Space of 2^n paths
                        * CFS "Frontier" determines a sort of contour of cost but with a similar
                        total number of nodes as BFS as Storage Space
                        * DFS goes all the way down levels of branches to leaf and then
                        back up working left to right, but at any point the "Frontier" has
                        only n nodes (instead of 2^n nodes). If also tracking the
                        "Explored Set" then do not get as much savings over BFS and CFS.
                * **Completeness Property of Algorithms**
                    * i.e. if there is a Goal State somewhere, will the algorithm find it?
                    * Given a state space of infinite trees where a Goal State is hidden deep down
                    in the tree then the algorithms guaranteed to find a path to the goal include:
                        * BFS, CFS, but NOT DFS
            * **Tree.Search Family of Algorithms - List of Algorithms**
                * **General Tree Search (Non-Graph Search)** using **Breadth-First Search (BFS) Algorithm**
                    ```
                    Tree.Search (problem p) returns path
                        frontier = { path(p.initial) }
                        loop:
                            if frontier is empty: return FAIL
                            path = remove.choice(frontier)
                            s = path.end
                            if Goal Test (s): return path
                            for a in p.Actions (s):
                                add [path + a > Result(s,a)]
                                to frontier
                    ```
                    * Steps
                        * Sequence of Paths 1:
                            * Pick Initial State with Path Length 0 (only path in Frontier, so is shortest)
                            * Expand Initial State, adding in all paths that result from applying possible Actions
                        * Sequence of Paths 2:
                            * Remove Sequence of Paths 1 from Frontier
                            * Find Shortest possible path resulting from next expansion
                                * Get all new paths resulting from applying possible Actions from Initial State
                                * If multiple new paths, then pick the shortest one
                                    * If all have same length, then break the tie at random, or use a technique
                        * Sequence of Paths 3:
                            * Remove other Sequence of Paths 2 from Frontier (i.e. keep only the shortest path)
                            * Add new paths to the Frontier, those that extend one level from new State (and including the
                             back-tracking path back to the previous state that is also now part of the Frontier)
                        * See **Graph Search** approach from here
                    * Note: Build a tree, always keeping track of the Frontier (ends of paths we haven't explored yet)
                    as the search space is explored, whilst behind the Frontier is the the set of Explored states,
                    and ahead is the Unexplored States
                    * Note: **Explored** States are tracked for when we want to expand and we find a duplicate,
                    so we can point back to a previously seen state (without having to create a new state in the
                    tree) and make a regressive step into the previously Explored state
                        * See slide "7.9. Tree Search Continued"
                * **Graph Search Algorithm** using **Breadth-First Search (BFS) Algorithm**
                    ```
                    Graph.Search (problem p) returns path
                        frontier = { path(p.initial) };
                        explored = { };
                        loop:
                            if frontier is empty: return FAIL
                            path = remove.choice(frontier)
                            s = path.end; add s to explored
                            if Goal Test (s): return path
                            for a in p.Actions (s):
                                add [path + a > Result(s,a)]
                                to frontier
                                unless Result(s,a) in frontier or explored
                    ```
                    * **Avoids the repeated paths** in Graph.Search function,
                    by modifying the Tree.Search function
                    * Steps

                        * Sequence of Paths 3:
                            * With **Graph Search**
                                * After we initialise "Explored Set" of paths already explored
                                * And after exploring new path, we add new state to set of "Explored Set"
                                * So when expanding the path and adding new states to end of it, we
                                **don't add an end if we've already seen that new state before in either
                                the "Frontier" or the "Explored Set"**
                        * Sequence of Paths 4
                            * When using **Breadth-First Search (BFS) Algorithm**, the paths
                            that are candidates to be explored next are the "shortest" ones
                            (i.e. length 1) that haven't already been explored yet
                            * When we try to add new paths to the Frontier from this "shortest" state,
                            if we find that those that extend one level from new State are either already in the
                            "Frontier" or the "Explored Set", then we won't add them, and
                            instead try the next "shortest" path, and try and add new paths from it to the Frontier
                        * Sequence of Paths 5
                            * Now with Frontier states that are all length of 2, we choose one of them,
                            * We then add state paths to the Frontier that expand one level from this chosen state
                            (but not the back-tracking path state since we're doing Graph Search),
                            and determine whether we terminate the
                            algorithm at this point (if we've reached Goal State) or if continue
                            * Note: The Goal Test isn't applied when we add a goal node path to the Frontier, but
                            instead when we remove a path from the Frontier. This is because it may not be
                            the BEST path to the Goal.
                                * Note: **Optimisation** may be made if using **Breadth-First Search** by
                                **changing the algorithm** and writing a **specific BFS Routine** so that
                                it checks the state as soon as they are
                                added to the Frontier and gives result as soon as the goal state is added to the
                                Frontier and terminates early (instead of after removing it from the Frontier by
                                waiting until they are expanded),
                                when we know there isn't the possibility of having a
                                shorter path to the goal of say length 2.5 instead of 3
                    * Note: **Breadth-First Search**
                        * Will find shortest path in terms of least number of steps
                        * Will NOT find shortest path in terms of shortest total cost
                        (by adding up the step costs), perhaps try **Uniform Cost Search** instead
                * **Graph Search Algorithm** using **Uniform Cost Search (UCS) Algorithm**
                    * Steps
                        * Initial State "Unexplored" - Step 0
                        * Change Initial State to "Explored"
                        * Expand out looking at an adding paths out of that State (i.e. 3 OFF) to the "Frontier"
                        * Pick path X to be expanded next (using UCS rules of "Cheapest total cost first")
                        * Remove that path X from the "Frontier" and add X to the "Explored"
                        * Expand out looking at an adding paths out of that State from X (without back-tracking) to the "Frontier"
                        and sum the total cost of those paths (across steps from initial state) and put on "Frontier" to compare
                        * Pick path Y branching from initial state to be expanded next
                        (using UCS rules of "Cheapest total cost first")
                        * Remove that path Y from the "Frontier" and add Y to the "Explored"
                        * Add new paths to neighboring/successor states from Y (without back-tracking) to the "Frontier"
                        and sum the total cost of those paths (across steps from initial state) and put on "Frontier" to compare
                        * Repeat picking path, etc
                        * Note, cannot terminate the algorithm until reach Goal State and have popped that cheapest path
                        off the "Frontier" (i.e. continue searching to see if a better cheaper path that also
                        reaches the Goal State)
    * Lab 7: Pac-Man
        * Help Pac-Man navigate his world in the most efficient way to
        find food and other objects by implementing BFS, DFS, and A\* Search.
        Opportunity to create a visually good app for portfolio.
        * Reference http://inst.eecs.berkeley.edu/~cs188/pacman/project_overview.html
        * Udacity Forums https://discussions.udacity.com/c/nd889-search-optimization/nd889-lab-pac-man

    * Lesson 8 - Simulated Annealing (in family of Local Search Algorithms):
        * Reading: AIMA Chapter 4.1, + 4.2-4.5
        * References: Different viewpoints and extensions
            * [Randomised Optimisations](https://classroom.udacity.com/courses/ud741/lessons/521298714/concepts/5344086080923)
        * Dfn: Used in AI when State Space is large and other
        techniques required to search it and find optimal solutions.
        * Example Problems: N-Queens problem where N Queens set up on a board
        in such a way that they cannot attack each other

        * **Iterative Improvement Problems and Algorithms**
            * Example #1: Travelling Salesman Problem
                * Problem:
                    * Given a Salesman
                    * Given 5x cities possible to visit
                    * Rules:
                        * Must visit all 5x cities before tour is finished
                        * Must return to starting city at finish
                    * Goal: Find most efficient order to visit the
                    cities to minimise overall distance flown
                    * Note:
                        * This is an **NP-hard** Problem
                        where all NP Problems are about as hard as each
                        other (exponentially difficult)
                            * N - Non-Deterministic
                            * P - Polynomial Time
                        * Use tricks to solve problem efficiently
                * Solution
                    * Approaches
                        * Randomly connect the cities
                        * Identify where paths cross
                            * **Iterative Improvement Algorithm** applied
                            by adding a small amount of intelligence to
                            (i.e. iteratively revising figure to uncross
                            each crossed situation to reduce distance
                            travelled) results in getting very close to
                            optimum solution (i.e. do this with 1000's of
                            cities and will get result within 1% of
                            optimum solution)
            * Example #2: Goodness graph
                * Problem:
                    * Given an N-dimensional x-y axis graph
                    that represents a Goodness Function that we are trying
                    to Optimise for a problem
                    * Find the Maximum point along it
                * Solution:
                    * Approach #1:
                        * **Hill climbing** - start somewhere (say right-most point)
                        and improve answer by travelling up the gradient
                        toward one of the maximum points, but the
                        problem is we may get stuck at a local maximum
                        when there may be multiple maximums.
                        Unstick from **Local Maximum** to find a
                        **Global Maximum** by using these techniques
                        * **Randomness Techniques** to use to get over problem
                        of getting stuck :
                            * **Random Restart**
                                * Start many particles at random positions on graph,
                                then Hill Climb, and take best results
                            * **Genetic Algorithms**
                                * Select positions based on Fitness Function
                                to breed children that mutate and eventually converge
                                on solution
                            * **Simulated Annealing**
                                * Analogy of gradual cooling temperature of particle
                                from very high temperature to make it move in
                                progressively less random ways while hill climbing
                                until converge on Global Maximum
                            * **Stochastic Beam Search**
                                * Combines the ideas of **Random Restart**,
                                 **Genetic Algorithms** and
                                **Simulated Annealing**, such that multiple particles
                                are released on graph and their children generate
                                progressively less randomly and eventually converge
                                on the maximum value


                        * Note that Minimax is only relevant to
                        Game playing, whilst UCS and BFS are difficult
                        to implement in 2D graphs since infinite
                        steps possible, and steps may need to be
                        be infinitely small to avoid skipping solution
            * Example #3: N-Queens Problem
                * Problem
                    * Given N queens placed randomly
                    on N*N chessboard in positions
                    so they cannot attack each other
                    i.e. only any one queen can be on any one
                    horizontal, vertical, or diagonal
                    * Find queen positions to reduce attacks down
                    to zero
                * Solution
                    * **N-Queens Heuristic Function**
                        * **Try the stupid thing first, then add
                        intelligence until solve the problem**
                        * Move queens to separate axis
                        * **Constraint Situation**
                            * i.e. only allow each queen to move up/down its column
                            (i.e. **Hill Climbing** so only allow move left/right along
                            x-axis of graph, so not too many pieces to move)
                        * Try Move queen subject to most attacks first
                        (move that improves situation the most)
                        * Test First Move - See how many attacks reduced to by
                        moving each of the queens to different positioins up/down
                        and comparing to find the lowest resulting attacks
                        * If more than one move reduces attacks by same amount
                        then choose one randomly
                        * Iterate until reach goal of 0 attacks
                        * **Local Minimum** If encounter situation where
                        No move decreases qty of attacks or increases them
                        (no move improves situation)
            * **Randomness Technique** - Other Usages
                * Particle Filters Technique
                * Pattern Recognition with Hidden Markov Models
                * Monte Carlo Markov Chains
            * **Random Restart (with use of Improvements)**
                * Given the following:
                    * **Objective Function (y-axis) in State Space (x-axis) Graph**
                        * Shoulder - positive gradient interrupted part-way with no increase
                        so can get stuck here like at Local Maximum
                        * Global Maximum - largest maximum in graph
                        * Local Maximum - smaller maximum along the graph
                        * "Flat" local maximum
                * Goal:
                    * Find Global Maximum and get Unstuck if encounter a Local Maximum
                    by using Random Restart
                * Steps
                    * Repeat the following:
                        * Randomly chooses a particle on the graph
                            * **Taboo Search Trick** of skipping to next if detect
                            is place we've been before since we are tracking prior places
                        * **Heuristic #1 - Large Step size and reduce over time**
                            * Start with Large Step size and decrease it over time to
                            ensure we reach the Global Maximum
                        * **Heuristic #2** - **Simulated Annealing** (Alternative to Heuristic #1)
                        * **Potential Issues**
                            * **Step Size Too Small Issue** Issue of landing on flat plateau
                            or Shoulder where not enough info available to determine
                            which way to go (to improve answer or check if reached maxium)
                            and may result in wandering
                            so need to only allow a Max amount of steps with no improvement in
                            score, but with small enough step size we may thing we're not
                            improving and that algorithm should stop
                            * **Step Size Too Large Issue**
                                * **Step across to different Hill**
                                Issue is that may miss intended hill
                                all together and go up a different hill instead with same incline
                                * **Infinite Loop Risk** since if step big enough to step over to other
                                side of hill where alternate incline direction and step big enough
                                over again and repeat (oscillating and not converge on answer)
                                    * Solution **Reduce Step Size** if detect any oscillations
                        * Follow up the positive gradient steps until reach Maximum
                        * Check if its the Global Maximum
                        * **Stage Algorithm Variant** Keep track of the current
                        found Maximum and try to generate shape
                        of the problem to predict where new Maximum may be
                        given places not yet explored
                        * Keep track place we have been before
                    * Find the Maximum of all the iterations
            * **"Simulated" Annealing**
                * Dfn: Repeat **Annealing process** according to a formula and timing
                to refining toward desired properties
                * Dfn: "Simulated" since we are borrowing ideas from physicists
                such as energy minimisation where:
                    * When external conditions may result in
                    molecules becoming mobile, and where their mobility slowly reduces,
                    they then arrange themselves into **Minimum Energy Configuration**,
                    which result in regular **Patterns**
                    * i.e. mud cracks where decrease in water in mud reduces molecule mobility
                    over time and causes mud cracks to change into regular **Patterns**
                    * i.e. iron molecules compact into **Minimum Energy Configuration**
                    forming lattice with other molecules like carbon
                    useful for sword making where a mixture of hardness and ductility is
                    needed (**Annealing process** heating to temp where atoms can move about and form
                    new structures with carbon and other molecules,
                    and cooling iron molecules repeatedly to so they preserve alignment
                    in desired lattice structures so swords are hard not brittle,
                    recrystalisation of sword making)
                    * i.e. honeycombs with **Minimisation by Design** where bees try
                    optimise storage space
                    the building materials used for the structure
                    * i.e. lava cooling slowly and rocks form
                    * i.e. columns of basalt rock where when it cools it shrinks and cracks
                    into hexagonal lattice which is a **Minimum Energy Configuration**
                * Note: Use idea of Heating and Cooling to get Unstuck from Local Minima
                toward finding Global Maxima, where the following applies until
                converge on solution:
                    * High Temperature equates to Higher Randomness
                    * Gradual Cooling results in Decreasing Randomness
                * Algorithm:

                    ```
                    for t=1 to inf do

                        # schedule of T values that is guaranteed to converge to the
                        # Global Maxiumum if it starts with very high values
                        # of T (temperature), causing lots of random graph motion as
                        # we take all random positions offered to us, even bad positions
                        # and climb up and down hills. then we'll decrease T slowly enough
                        # until it's very small with no randomness where we just
                        # Hill Climb to nearest peak normally
                        #
                        # when temperature T is VERY HIGH approaching infinity
                        # then probability (e^(change_in_E/inf) = e^0 = 1) approaches 1
                        # (even if change_in_E is negative)
                        #
                        # when T is VERY SMALL small (opposite situation) normal Hill Climbing occurs (instead of random)
                        #
                        # given temperature T approaching 0
                        # T = 0.01 (i.e. near 0)
                        # note: do not want T to be precisely 0 as will give undefined answer
                        #
                        # if new random pos. improves score E then take it (up the gradient)
                        # if new random pos. does not improve E then change_in_E = 0
                        # if new random pos. worsens score then change_in_E < 0 (down the gradient)
                        #   e^change_in_E/T = e^(-1/0.01) = e^-100 (very small number)
                        #   (so almost no chance of taking this suggested new random pos.)
                        #
                        # if get stuck at a flat plateau (i.e. shoulder) where change_in_E == 0, and regardless of T's value,
                        # then e^0 = 1 and the algorithm will revert back to taking new random positions again
                        # until walk off the plateau to a position with positive gradient that can improve E toward maxium peak
                        #
                        # similarly in "real-world" annealing, when temperature
                        # is very high the particles jump around a lot at the
                        # start so that if the particle gets stuck at a
                        # Local Maxium instead, then given sufficiet time and
                        # randomness it has the ability to leave the
                        # peak and (ignore the fact that it is going the wrong way
                        # and ending up at different part of the graph).
                        # since temperature T is high, the next fixed random point
                        # may make it move both down and up the slope until it
                        # hits the Global Maximum

                        T <- schedule (t)
                        if T=0 then return current

                        # select points randomly in nearby region

                        next <- a randomly selected successor of current

                        # select points if we find with improved value

                        if change_in_E > 0 then current <- next

                        # select points also if poorer value but only with
                        # a probability

                        else current <- next only with probability e^(change_in_E/T)
                    ```
            * **Local Beam Search**
                * Given a graph, uses k multiple particles (positions)
                instead of just one
                * At each time-frame inspect all randomly generated neighbors
                of each particle and compare them with each other and retain
                the k best particles for the next iteration.
                * Shares information between each particle and iteration, which differs from
                Normal **Random Restart** that doesn't share any info between iterations
                * Terminate if any particle reaches a Goal State
            * **Stochastic Beam Search**
                * Combines the ideas of **Random Restart**, **Genetic Algorithms** and
                **Simulated Annealing**, and
                * Same as Local Beam Search but more useful, since successors are
                chosen not just based on fitness, but also based on Randomness
                so we don't get stuck in a Local Maximum (similar to Simulated Annealing
                within class of Genetic Algorithms)
            * **Genetic Algorithms** (aka, second best solution to ANY problem)
                * Dfn: Genetic Algorithms is analogy to Natural Selection in Biology
                that uses Breeding and Mutation to find Optimal answer to problem
                (fancy version of Stochastic Beam Search that happens to make a nice
                analogy with biology)
                * **Reducing Randomness over time** (like in Simulated Annealing)
                    * **Fitness Function** reduces the Randomness as each Generation
                    gets closer to solution
                    * Additional measures to Optimise and converge quickly such as:
                        * **Tuning Parameters**
                            * Crossover
                            * Mutations qty
                            * Parents qty
                * **Convention to represent given board**
                    * Given one Queen in each column, encode a board with a number
                    shown below each column that indicates where Queen is
                    (i.e. grid distance of the Queen from the bottom)
                    (i.e. 24748552, for 8x8 board)
                    * Given 8x8 board, where there are 8 Queens, then after examining
                    every possible pair of Queens that could attack each other we find there are
                    28 pairs.
                        ```
                        8 choose 2 = ( 8! / (8-2)!*2! ) = 28
                        ```
                    * Goal is to reduce qty of attacking pairs of Queens to 0,
                    so use **Fitness Function** as follows, such that when it is
                    equal to 28 we know we have Won:
                        ```
                        Fitness Function for board
                            = Max qty of attacking pairs of Queens - qty of attacking Queens for given board
                            = 28 - qty of attacking Queens for given board
                        ```

                * **N-Queens Problem using Genetic Algorithms**
                    * **Gene Pool** Choose 4x random boards
                    (i.e. 24748552, ..., ..., ...)
                    * Combine Genes from Gene Pool in different
                    Patterns to Breed better boards
                    * Choosing Genes from Gene Pool
                    * Evaluate each board state in initial population
                    using **Fitness Function** to get a
                    Fitness Score Value (aka qty of non-attacking pairs of Queens)
                    for each is calculated by going through each column from left-to-right
                    and looking only to the right for each Queen to see if it attacks
                    another (to prevent duplicates)
                    * Proportional Probabilities (%) based on the Fitness Scores
                    indicative of how likely each board is to be chosen as
                    a Parent and breed, when then list them in descending order of %
                    (i.e. add the 4x scores and Normalise each into a percentage)
                    * Randomly Select 4x Parents from the 4x Boards and order from best
                    to worst (best on top)
                    (i.e. given 4x boards with 31%, 29%, 26%, and 14% respectively,
                    by rolling a 100 sided dice to select 1st Parent, and if it
                    rolls between 1 and 31 we select the 1st board, 32 to 60
                    (i.e. 31% + 29% = 60%) we select
                    2nd board, 61 to 76 we select the 3rd board, and 77 to 100 we
                    select the 4th board)
                    * Create 2x Children from each of the 4x Parents using a
                    **Crossover Process** (see Lesson 8.21) where we group parents
                    into Pairs
                    and pick a random vertice (**Crossover Point** b/w two columns)
                    for each Pair. Create the 1st Child
                    by combining the LHS of the vertice of the Parent 1 with the
                    RHS of the vertice of Parent 2, and repeat by
                    combining LHS of Parent 2 with RHS of Parent 1 (for 1st Pair),
                    and repeat for remaining Pairs... This way at least one of the
                    Children gets the Best attributes of the Parents
                        * Change **Parameters** in Genetic Algorithms to try and
                        Optimise how quickly we converge to a good result,
                        such as the qty of Children that each Parent may create
                    * Note: Survival of the fittest, as with increasing amount
                    of Generations of Children eventually we evolve to an
                    8-Queens game board that solves the problem,
                    the ones with lesser attacking Queens
                    will have Higher Fitness Function and higher chance to create
                    children, whereas the one with higher attacking Queens will
                    have a Lower Fitness Function and less chance of creating children
                    * **Genetic Algorithms Mutation**
                        * **Problem**
                            * Not selecting a Parent to breed where
                            Fitness Function gives it Low Proportional Probability,
                            but where that Parent has a critical piece, but never
                            returns as a result of breeding from the other Parents
                        * **Solution**
                            * **Genetic Mutations** to add Randomness after Crossover
                            with a Mutation step that avoids risk of never reaching goal state
                            (similar to occasionally choosing a random direction
                            in Simulated Annealing, and similar to randomness in
                            Stochastic Beam Search),
                            whereby
                            for each qty (digit) of attacking queens for a given board,
                            we have a small chance by significant chance that the digit
                            will mutate into a different digit
    * Lab 8 - Simulated Annealing
        * Discussion Forums https://discussions.udacity.com/c/nd889-search-optimization/nd889-simulated-annealing
        * Use simulated annealing to implement the algorithm in a Jupyter notebook
        and using it to solve the Traveling Salesman Problem (TSP) between US state
        capitals.
        * Steps
            * Fork https://github.com/udacity/AIND-Simulated_Annealing
            * Open AIND-Simulated_Annealing.ipynb with:
                `jupyter notebook AIND-Simulated_Annealing.ipynb`
            * Follow instructions from AIMA to implement Simulated Annealing
            in the Jupyter Notebook
                * Simulated Annealing pseudo-code https://github.com/aimacode/aima-pseudocode/blob/master/md/Simulated-Annealing.md
                * Simulated Annealing code https://github.com/aimacode/aima-python/blob/master/search.py
                    ```
                    def simulated_annealing(problem, schedule=exp_schedule()):
                        """[Figure 4.5] CAUTION: This differs from the pseudocode as it
                        returns a state instead of a Node."""
                        current = Node(problem.initial)
                        for t in range(sys.maxsize):
                            T = schedule(t)
                            if T == 0:
                                return current.state
                            neighbors = current.expand(problem)
                            if not neighbors:
                                return current.state
                            next = random.choice(neighbors)
                            delta_e = problem.value(next.state) - problem.value(current.state)
                            if delta_e > 0 or probability(math.exp(delta_e / T)):
                                current = next
                    ```
            * Install Jupyter Notebook with Miniconda
                *  http://jupyter.readthedocs.io/en/latest/install.html
                * `pip3 install --upgrade pip`
                * `pip3 install jupyter`
    * Lesson 9 - Constraint Satisfaction:
        * Reading - AIMA Chapter 6
        * **Techniques**
            * Implement these algorithms optimise searching for solutions in
            constraint satisfaction problems in the following order, starting with
            the stupidest one first, then add intelligence as required
                * **Backtracking Search** (aka Simple Search) (i.e. stupidest algorithm)
                    * i.e. States are defined by values assigned so far
                    * Initial State is empty assignment
                    * Check if current assignment is goal
                    * Assign a value to unassigned variable that does not conflict with current assignment
                    * Dead End is reached when no legal assignments (if so, backtrack to previous
                    state and try different assignment)
                    * Repeat recursively until find answer or have tried all possible assignments and
                    reported failure
                * **Forward Checking**
                    * Tracking remaining legal values for each unassigned Variable,
                    so if ever make decision causing unassigned Variable to not
                    be able to have a value, we stop search and backtrack
                    * Benefit: If stuck (i.e. can't assign a colour to a territory)
                    can backtrack earlier than would have if continued
                    assigning variables (i.e. early warning system that our search is going
                    down the wrong branch)
                    * Weakness: Does not provide early detection for all failures
                    (i.e. detect when two unassigned Variables are adjacent and only the
                    same Domain is available to populate them with, so we need to use other
                    Constraint Propagation repeatedly to enforce all local constraints)
                * **Arc Consistency (aka **Simple Constraint Propagation)**
                    * A Variable and a Constraint Propagation Problem are considered
                    Arc Consistent wrt another Variable if value still available to
                    Second Variable after assign value to First Variable.
                    Network is **Arc Consistent** if all Variables in graph satisfy this condition.
                    * Benefit: Potentially saves a lot of unncessary deep searching for more
                    complicated problems
                    * Example (see Map Colouring example below)
                * Heuristics
                    * **Minimum Remaining Values (MRV)** (see Map Colouring example below for explanation)
                    * **Degree Heuristic** (see Map Colouring example below)
                    * **Least Constraining Value**
                        * Assign the Domain to
                        a Variable that least constraints future choices (i.e. reuse previously
                        used Domains unless absolutely necessary to use a new one) to avoids
                        Dead Ends and Backtracking
                        (i.e. rules out the fewest values
                        in the remaining Domains to improve Backtracking Search
                        algorithm efficiency)
        * Example:
            * Airport departure scheduling:
                * Given:
                    * 2500 arrivals / day
                    * 2500 departures / day
                    * Plane departs every 30 sec
                * Problem:
                    * How schedule all the flights?
        * Example: Crypto-arithmetic Q1
            * Given
                * Each letter represents a digit
                * Combination of all letters in word represent a different digit
                * None start with 0
            * Setup problem by listing
            * Variables         `F T U W R O X1 X2 X3`
            * Domains           `{ 0,1,2,3,4,5,6,7,8,9 }`
            * Constraints/Rules: `alldiff { F, T, U, W, R, O }`
                * Global Constraints
                    * 1) All letters different. No letter can represent same digit
                    * `O + O == R + 10*X1`
                    (i.e. except when carry to next column X1 in 10ths place)
                        * where X1 == 0 || 1
                    * `X1 + W + W == U + 10 * X2`
                        * where X2 is 100ths place
                    * `X2 + T + T == O + 10 * X3`
                        * where X3 is 1000ths place
                    * `X3 == F`
                    * `F != 0` (since problem does not allow leading 0's)

                * Preference Constraints
                    * Unary Constraints
                        * Show constraints as square boxes on
                        **Constraint Hypergraph**
                            * Top square represents Global Constraint 1)

            * Use procedure of **Backtracking Search** improved by using
            WITH **Minimum Remaining Values (MRV) Heuristic**, **Degree Heuristic**, and
            **Least Constraining Value Heuristic** to minimise backtracking)
                * Given Variables, Domains, and Constraints
                * Select unassigned Variable (i.e. X3), with Domain of 0 or 1
                * Assign Variable X3 with a Domain value (i.e. 1),
                since can't choose 0 as would fail constraint and
                not pass Forward Checking
                * Apply **MRV Heuristic** and select unassigned Variable (i.e. F)
                since only has 1 remaining value and assign it with Domain value 1
                * Now, **MRV Heuristic** find that X2 and X1 are tied with min remaining values
                at 2 (i.e. 0 or 1), so choose 0 for both (either value passes Forward Checking)
                * Now, O must be 4, since;
                    ```
                    `O + O == R + 10*X1`
                    `2*O == R
                    `O == R/2`
                    so O is even since R max is 8, such that O == 4
                    ```

                    ```
                    `X2 + T + T == O + 10 * X3`
                    `0 + 2*T == O + 30
                    `2*T - O == 30`
                    `T == 15 + (O/2)`
                    so if O == 4, then T == (1)7 == 7 (passing 1 to F)
                    ```

                    ```
                    `X1 + W + W == U + 10 * X2`
                    `U == 2*W`
                    so, U must be even number less than 9, so
                    choose U value of 6 (passes Forward Checking)
                    ```



            * Outcome (see lecture notes for details https://www.youtube.com/watch?time_continue=97&v=iqJbOGWEd6Y)
                * X3: {0,1} => X3 = 1
                * X2: 0
                * X1: 0
            * Answer i.e. F=1, O=4, R=8, T=7, U=6, W=3
            ```
              TWO
            + TWO
            =====
             FOUR
            ```

        * Example: Map Colouring
            * **Constraint Optimisation Problem** since has Preference Constraints, and is solved with
            Linear Programming, Factory Job Scheduling
                * e.g. Map Colouring, Sudoku, Car Assembly, Class Scheduling, Spreadsheets,
                Transport Scheduling, Floor Planning, N-Queens (1000-Queens)
            * Given
                * Add colours to each territory in Australia
                * Ensure no neighboring territory use same colour
            * Setup problem by listing:
                * Variables             `WA, NT, Q, NSW, V, SA, T`
                * Domains               `Di = { green, blue, orange }`
                * Constraints/Rules:    adjacent regions must have different colours
                                        (i.e. list all possible pairwise constraints `WA != NSW`)
                                        OR (i.e. list all allowable assignments for each pair of territories
                    * Preference Constraints
                        * **Unary Constraints** (restrict value of given variable) (i.e. `WA not orange`)
                        * **Binary Constraints** (relate two variables at most)
                            * **Constraint Graph** uses for representation visually
                                * Nodes - (i.e. variables `WA, NT, ...`. Note `TAS` is independent from rest of problem
                                * Lines - show constraints between variables)
                                * Apply **General Constraint Satisfaction Problem Algorithms** to the Constraint Graph
                                Structure to search quickly for answer
                        * **Multiple Constraints** (i.e. 3 or more variables)
                        * **Soft Constraints** (i.e. prefer to teach on Tues and Thu)
            * Solution:
                * An assignment of colours to each of the territories that satisfies all constraints
           * Concrete Example Solution (i.e. using **Backtracking Search** ONLY)
                * Given Variables, Domains, and Constraints
                * Select unassigned Variable (i.e. `WA`)
                * Assign Variable a Domain `orange` from possible (i.e. `green, blue, orange`)
                * Select unassigned Variable (i.e. `NT`)
                * Assign Variable a Domain `blue` from possible (i.e. `green, blue`)
                * Select unassigned Variable (i.e. `QLD`)
                * Assign Variable a Domain `blue` from possible (i.e. `blue, orange`)
                * **Dead End** when try unassigned Variable (i.e. `SA`)
                * **Backtrack** up to previous branch where only `orange` and `green` had been used
                * Continue until find solution
                    * Improve efficiency of algorithm
           * Concrete Example Solution (i.e. **Backtracking Search** improved by using
           WITH **Minimum Remaining Values (MRV) Heuristic**, **Degree Heuristic**, and
           **Least Constraining Value Heuristic** to minimise backtracking)
                * Given Variables, Domains, and Constraints
                * **Structured CSPs Algorithm (Optimisation)**
                    * Reference: **Graph Algorithm**: Topological Sorting
                    https://courses.cs.washington.edu/courses/cse326/03wi/lectures/RaoLect20.pdf
                    * Benefit: Save time when deep searching
                    * Approach: Investigate "Structure" of problem and decompose
                    into independent sub-problems (i.e. `TAS` is separate), which
                    removes a level from the search tree
                    * Example 1:
                        * Given problem with:
                            * n = 80 possible Variables
                            * d = 2 possible values each (Domain makes Variables binary in nature)
                            * c = 20 possible Variables in each sub-problem
                            (after dividing into 4x problems of 20 each)
                            * d^n = 2^80 (original Search Space)
                                * n/c * d^c = 4 * 2^20 (afterward Search Space)
                    * Example 2: **Tree-Structured CSPs Algorithm (fast method, but must convert
                    Constraint Graph into a tree first**
                        * Given problem with No Loops, can solve problem
                        in O(n*d^2) Time instead of O(d^n) Time
                        (where n is qty Variables, and d is size of Domain) by:
                            * If Constraint Graph is not a "tree", then try to modify the problem
                            by assigning a Domain to some Variables first in order to
                            change the Constraint Graph into a "tree"
                            (i.e. if use MRV first and assign Domain to unassigned Variable
                            so that Variable is removed from the possibilities of its neighbors
                            then the problem becomes a "tree" and can be solved using this fast
                            method)
                            * Choose a Variable as root
                            * Choose an ordering of Variables from the root to the leaves such
                             that each Variable node appears after its parent node in the tree
                            * Start at end Variables (leaves) and make each parent **Arc Consistent**
                            going up the tree until we get to the root, and if not possible,
                            report failure of the problem
                            * Start at root of tree and choose any Domain assignment
                            available until we get to the leaves (since when the tree is
                            **Arc Consistent** any of the available assignments will solve the problem)

                * **Forward Checking** Status:
                    * All territories could have any of the 3x possible colours
                * Select unassigned Variable (i.e. `WA`)
                * Assign Variable a Domain `orange` from possible (i.e. `green, blue, orange`)
                * **Forward Checking** Status Update:
                    * All territories could have any of the 3x possible colours,
                    EXCEPT:
                        * Orange removed from colours available to NT and SA
                * Select unassigned Variable (i.e. `NT`)
                * Assign Variable a Domain `green` from possible (i.e. `green, blue`)
                * **Arc Consistency (aka Simple Constraint Propagation check)**
                    * When assigning `green` to NT, ensure **Arc Consistency** by potentially
                    causing a chain of changes to nested neighbors. First checking NT's
                    unassigned Variable primary-neighbors
                    (i.e. QLD, SA) and check if assignment of "green" reduces qty of
                    Domains available to them, and if so, remove "green" from the possible Domains
                    available to each of them, then check the unassigned Variable secondary-neighbors
                    and so forth to propagate remaining Domains. Repeat until
                    no more unassigned Variables, or if a neighbor is found to not have any
                    remaining Domains available in which case we force the search to try another option
                    for NT.
                    Note: Entire region of combined territories is **Arc Consistent** if
                    there are no regions without a possible colour
                * **Forward Checking** Status Update:
                    * etc
                * Optimise and improve Backtracking efficiency by choosing next
                Variable using a Heuristic as follows:
                    * **Minimum Remaining Values (MRV) Heuristic** to assign Domain to a
                    Variable that has fewest legal moves (i.e. remaining unused Domains)
                        * i.e. Select unassigned Variable `SA`
                    * **Degree Heuristic** used if there is a Tie when performing MRV,
                    and involves choosing Variable with most Constraints on remaining
                    Variables (i.e. has the most borders with other territories so is most complex)
                    and we will run into problems sooner rather than delaying them
                * Optimise and improve Backtracking efficiency by choosing next
                Domain using a Heuristic as follows:
                    * **Least Constraining Value Heuristic** used to assign the Domain to
                    a Variable that least constrains future choices (i.e. reuse previously
                    used Domains unless absolutely necessary to use a new one) to avoids
                    Dead Ends and Backtracking
                        * i.e. Assign Variable a Domain `orange` again as it least
                        constraints future choices (leaving `blue` unused)
        * Example: N-Queens (Constraints Satisfaction Problem)
            * **Iterative Improvement Algorithms**
                * Examples: Genetic Algorithms or Hill Climbing Algorithms
                * Most efficient when either:
                    * Many solutions OR
                    * Few solutions
                * Very Hard (requiring significant CPU time)
                to solve at a Critical Ratio point between
                qty of Variables for a given qty of Constraints
                * Solved by randomly assigning values to Variables and
                iteratively improve the assignments until reach solution.
                **Minimising Conflict Method**
                of minimizing qty of attacks with each
                iterative improvement can find solution for 4-Queens
                problem which has 4^4 = 256 possible states.
                It can also solve 1,000,000-Queens in average of 50 moves
                (where large qty of solutions to problem)
    * Lab 9 - Constraint Satisfaction
        * Discussion Forums https://discussions.udacity.com/c/nd889-search-optimization/nd889-constraint-satisfaction
        * 8-Queens Puzzle Solution
            * Constraint Satisfaction Problem of solving N-queens problem using symbolic constraints
            and backtracking search in a Jupyter notebook.
        * Steps
            * Clone https://github.com/ltfschoen/AIND-Constraint_Satisfaction
            * Open AIND-Simulated_Annealing.ipynb with:
                `jupyter notebook AIND-Constraint_Satisfaction.ipynb`
            * Follow the instructions to complete the constraints for the N-Queens CSP
            and implement Backtracking-Search from AIMA in the notebook.
                * CSP pseudo-code https://github.com/aimacode/aima-pseudocode/blob/master/md/Tree-CSP-Solver.md
                * CSP code https://github.com/aimacode/aima-python/blob/master/csp.py

            * Install Jupyter Notebook with Miniconda
                *  http://jupyter.readthedocs.io/en/latest/install.html
                * `pip3 install --upgrade pip`
                * `pip3 install jupyter`

    * Lesson 10 - Logic and Reasoning:
        * Dfn:
            * Logic - A logic is a way of describing the world. If logical statements
            about world selectively capture what want to know about world correctly, then
            we may reason within the logic to come to conclusions (without having to reference
            the world again)
        * AI Background
            * 50's & 70's - Requirement for better languages and systems for logical statements
                * Mistakes:
                    * Trying to make statements only in Boolean Logic (where only True or False)
                    even though world is uncertain and Probability Logic is better fit with world
            * 80's - Better algorithms for Probability Logic
                * Issues
                    * Too laborious writing by hand
            * 90's - More data available online
                * Opportunities
                    * Learning about world from data instead of writing rules by hand
        * **Uncertainty Domains**
            * Goal: Eliminate uncertainty of real-world and solve significant problems using logic
            * Future of Planning Algorithms:
                * Learning from examples
                * Transferring learning across domains (i.e. learn in one domain and execute in another)
                * Interactive Planning where human-machine team solves problems together
                and being able to explain things in terms that a human can understand,
                allowing humans to add things that were overlooked in the original problem statement
                (i.e. need correct UI interface as well as correct answer)
            * Examples:
                * Logistics
                    * **Planning Algorithms** for when actions or world is uncertain such as
                    for all deliveries of packages using a large fleet of vehicles
                        * Describe as logic problem where want find solution minimising time and expense
        * **Valid Sentence**
            * Dfn: True in every possible Model for every combination of values of Propositional Symbols
        * **Satisfiable Sentence**
            * Dfn: True in some Models, but not necessarily all Models
        * **Satisfiable Sentence**
            * Dfn: False for all Models
        * **Logic Types**
            * **Propositional Logic**
                * **Truth Tables for all Logical Connectives**
                    * Lists all four possibilities for combinations of Propositional Symbols
                    * Note: "v means either OR both"
                    * Note: "> means P implies Q" (aka if P then Q)
                    * Note: Meaning in Logic is defined explicitly by Truth Table
                ```
                P  Q  !P  P ^ Q  P v Q  P > Q  P <> Q
                F  F  T   F      F      T      T
                F  T  T   F      T      T      F
                T  F  F   F      T      F      F
                T  T  F   T      T      T      T
                ```
                * Example:
                    * Given the following propositions
                        * [O] means Five is odd number
                        * [P] means Paris is capital of France
                        * [E] means Five is even number
                        * [M] means Moscow is capital of France
                    * Connectives:
                        * O > P (i.e. IF Five is odd number THEN implies Paris is capital of France)
                                      IF True THEN implies True
                        * E > M (i.e. IF Five is even number THEN implies Moscow is capital of France)
                                      IF False THEN implies False
                            * Note: In terms of real-world formal logic both statements are True
                            since according to the **Truth Table** definition,
                            P > Q is True when both P and Q are False
                * Example: Alarm Problem
                    * Given propositional Symbols B, E, A, and M, where our belief in
                    Propositional Logic is that each is either True, False, or Unknown
                    (similar to in Probabilistic Models, but dissimilar in that
                    our degree of belief in Propositional Logic is not a number).
                    They correspond to proposed Events:
                        * [B]urglary occurring
                        * [E]arthquake occurring
                        * [A]larm going off
                        * [M]ary calling
                        * [J]ohn calling
                    * **Propositional Logical Sentences** are either True or False with respect
                    to a Model of the world (a set of True/False values for all the
                    Propositional Symbols), (i.e. **Model** may be the set {B: True, E: False}).
                    **Truth Tables** used to define Truth of a Sentence in terms of the
                    Truth of the Symbols with respect to the Models.
                    They may be created by combining these Symbols with
                    Logical Constants of True and False, and Logical Operators, where
                    **Connectives** include: "^ means AND", "v means OR", "> means Implies",
                    "<> means Equivalent" (biconditional)" (when one true the other is true, and when
                    one is false the other is false), and "! means Negation"
                        * (E v B) > A   (i.e. imply Alarm is True whenever Earthquake and Burglary is True)
                          i.e. E OR B == A
                        * A > (J ^ M) (i.e. imply both John and Mary call when Alarm is True)
                          i.e. A == J AND M
                        * J <> Mary (i.e. John calls if and only if (iff) Mary calls, so John is equivalent to Mary)
                          i.e. J == Mary
                        * J <> !M(i.e. when John calls Mary does not call, and vice versa, so John is equivalent to not Mary))
                          i.e. J == !M
            * **Propositional Logic Limitations**
                * Inference Mechanisms that are efficient at determining **Validity** and **Satisfiability**
                * Limitations
                    * Only able to handle True and False values (in Truth Table), with no
                    capability to handle **Uncertainty** like covered in **Probability Theory**
                    * Only talks about True or False events in the world
                        * Does not talk about objects with properties such as size, weight, etc.
                        * Does not talk about relations between objects
                        * Does not include any shortcuts to succinctly talk about multiple things happening
                        (i.e. vacuum world with multiple 1000 locations, to say every location
                        is free of dirt, we need a conjunction of 1000 propositions,
                        but **can't** just write a single sentence saying all locations are clean)
            * **First-Order Logic (FOL)**
                * Dfn: **First-Order Logic** means that relations are on Objects but
                not on Relations
                    * Note: Every model must have at least one object, and multiple variables
                    may refer to the same object
                    * Note: P(x) means there may exist an x that is a member of P, where
                    P may be an empty relation
                    * Note: Don't just have Sentences with an assertion, always include a
                    condition (If and Only If), so can prove the negative of the assertion
                    without just an implication in one direction,
                    when doing the definition of an adjacency or a membership problem
                * Dfn: **Higher-Order Logic (HOL)** means that relations are on Relations
                    * i.e. define notion of a transitive relation about relations itself
                    * Valid statement in Higher-Order Logic (but invalid in FOL:

                    ```
                    For All R, transitive of R is equivalent to, ...
                    For All a,b,c, R(a,b) and R(b,c) implies R(a,c)

                    ∀R Transitive(R) <=> (∀a,b,c R(a,b) ^ R(b,c) => R(a,c))
                    ```

                * About: Overcomes Limitations encountered in Propositional Logic

                    ```
                    Logic varies in what you can say about the world, and what you can believe
                    about what's been said about the world

                                            World                                   Belief
                                            (Ontological Commitment)                (Epistemological Commitments)
                    ======================================================================================
                    First Order Logic       Rel'ships, Obj's, Functions on Obj's    True, False, Unknown (?)
                    (extension on Prop logic)
                    Propositional Logic     Facts (Symbols, Variables)              True, False, Unknown (?)
                    Probability Theory      Facts (")                               [0..1] (real numbers)


                    Atomic           Factored                       Structured
                    ===========================================================

                    Search &
                    Problem Solving  Variables                      Relations b/w Objects
                                     (i.e. Propositional Logic &    (i.e. Programming languages, SQL)
                                      Probability Theory)
                    ```

                    * **Atomic Representation**
                        * Dfn: where representation of state is just individual state
                        when no piece is inside it (no internal structure to states)
                        * i.e. given transition from State A to State B
                        can say if identical or not, if goal state?
                        * Examples: Search, Problem Solving
                    * **Factored Representation**
                        * Dfn: break up world into set of True/False facts
                        * Dfn: representation that individual state of world is factored into
                        several **Variables** (i.e. B, E, A, M, J), i.e. boolean or other type
                        of representation, but Not in Propositional Logic
                        * Examples: Propositional Logic, Probability Theory
                    * **Structured Representation**
                        * Dfn: Most complex representation. Individual state is not just
                        a set of values for variables, but also includes rel'ships b/w
                        objects, a branching structure, complex representations, relations
                        b/w different objects
                        * Examples: Programming languages, structured databases (with SQL over them)
                * Process
                    * Definitions
                        * **Function** defined as mapping from objects to objects
                    * Model in FOL **differs from Propositional Logic Model**, where value for each Propositional Symbol in
                    Model corresponding to what is going on in a possible world
                        i.e. { P: True, Q: False }
                    * **Model in FOL** instead starts with Set of Objects and
                    Set of Constants that refer to those Objects
                    but don't need one-to-one correspondence b/w Constants and Objects
                    (i.e. multiple Constants may refer to same Object,
                    for instance CEE could refer to Object C3)

                        * Set of Objects include: 4x tiles  A1 C3
                                                          B3 D2
                              Note: with numbers 1,2,3 that are also Objects in Model

                        * Set of Constants:       {A, B, C, D, 1, 2, 3, CEE}

                        * Set of Functions:

                            "Number" Function (i.e maps from tile to number on the tile)
                                Defined as mapping:

                                                { A -> 1, B -> 3, C -> 3, D -> 2 }

                        * Set of Relations:

                            "Above" Relation (i.e. where in this model of the world, this
                                              relation is a set of two pulls where
                                              A is above B, and C is above D)
                                  Note: A binary relation holding b/w two objects, say where
                                  one block is above another block

                                                { [A, B], [C, D] }

                            "Vowel" (Unary Relation)
                                        if Vowel is True only for the Object A,
                                        then that's a set of Tuplies of length 1 containing just A

                                                { [A] }

                            "Rainy" (Relation over no objects, just refers to current situation)
                                        if not rainy, represent as empty set of no tuples:

                                                { }

                                        if is raining, represent with Singleton Set,
                                        and since Arity of rainy is zero, there'd be
                                        zero elements in each of the tuples
                                                { } { [] }

                    * **Syntax in FOL**
                        * **Atomic Sentences** - describe facts as either True or False
                        and are predicates corresponding to Relations
                            i.e.
                            ```
                            Vowel(A)
                            Above(A, B)
                            2 = 2          (i.e. Equality Relation in every model to distinguish)
                            ```
                            * **Operators** Sentences may be combined with all operators from
                            Propositional Logic (i.e. ^, v, !, >, <>, () )
                            * **Quantifiers** Complex Sentence Types (unique to FOL)
                                * ∀x - For All, followed by Variable it introduces such as x
                                * Ǝy - There Exists, followed by Variable it introduces such as y
                                * Note: If no Quantifier provided, its a shortcut meaning For All
                                * Note: Typically don't want to say something about every object in
                                the domain since objects can be so different, instead
                                we usually want to say something about a particular type of object
                                (i.e. Vowels)
                                * Note: Typically when have a "There Exists an x" or any other
                                Variable, it typically doesn't go with a Conditional since
                                only describing one object

                            * Example Sentences Typical in FOL:
                                `∀x Vowel(x) => Number of (x) = 1`
                                (i.e. For All x, if x is Vowel, then Number of x equals 1)
                                `Ǝy Number of (x) = 2`
                                (i.e. There Exists an x such that the Number of x equals 2)
                                (means there is some Object in the Domain to which the Number of
                                 Function applies and has a value of 2, but without saying
                                 what the Object is)
                        * **Terms** - describe Objects, may be Constants, Variables, or Functions
                            i.e.
                            ```
                            A,B,2         (i.e. Constants)
                            x, y          (i.e. Variables)
                            Number of (A) (i.e. Functions)
                                i.e. just another expression referring to same object as 1
                            1             (least in model shown previously)
                            ```
                * Example: Vacuum world represented using FOL
                    * Add:
                        ```
                        Locations
                        =========
                        A - Left Location
                        B - Right Location
                        V - Vacuum
                        D1, D2 - Dirt

                        Relations
                        =========
                        Loc - True of any location
                        Vacuum - True of the vacuum
                        Dirt - True of dirt
                        At(o,l) - True of object and location
                        ```
                    * i.e. Say "vacuum is at location A":
                        `At(V,A)`
                    * i.e. Say "no dirt at any location";
                        * Note: This Sentence still holds for 1000's of locations,
                        which is the **power of FOL**
                        ```
                        For All dirt, and For All locations, if D is Dirt,
                        and if l is a location, then d is not at l.

                        ∀d ∀l Dirt(d) ^ Loc(l) => ¬At(d,l)
                        ```
                    * i.e. Say "vacuum is in a location with dirt" (without specifying the location)
                        ```
                        There Exists an l, and There Exists a d, such that d is the Dirt,
                        and l is the Location, and the vacuum is at the location,
                        and the dirt is at that same location

                        Ǝl Ǝd Dirt(d) ^ Loc(l) ^ At(V,l) ^ At(d,l)
                        ```


        * **Expert Systems**
            * Dfn:
                * Export Systems are common throughout the world
                * Algorithms to help understand how Export Systems work
            * Examples:
                * Decisions
                    * Superhuman decision made by military expert system such as deciding
                    where to place a supply base to save money (higher return than
                    their investment in AI)
        * **Resolution Algorithm** - Inferring new knowledge from a knowledge base
        * **GraphPlan** - Used to make planning practical for new range of problems
        * **Value Iteration Concept** - Key concept for **Markov Decision Processing**

    * Lesson 11 - Planning:
        * About:
            * Problem Solving AI-agent
                * Given a State Space, Start Space, and Goal
                * Planning ahead, thinking only to figure out path to find goal, without interacting/sensing world
                * Executes path
            * Real-Life differs from Problem Solving AI-agent and requires:
                * Avoiding blindfolded execution and getting lost by:
                    * **Interleaving** planning and executing
                    * Feedback from environment required, cannot just plan ahead and derive whole plan
        * **Planning and Executing**
            * **Interleaving** required due to Properties of the environment that make it difficult:
            * **Change Perspective** to overcome Difficulties by planning in
            **Space of Belief States** instead of **Space of World States**
            * **Difficulties**
                * **Stochastic Environments** - don't know outcome of performing an action, so need to
                  deal with **Contingencies**,
                    i.e. try turn right and wheel slipped, skidding, feet not move forward entirely straight,
                    or traffic lights red (prevents go forward)
                * **Multiagent Environments** -
                    i.e. other cars or people obstacles, plan about what they will do
                    and react to unexpected (only know at **Execution** time, not at **Planning** time)
                * **Partial Observability**
                    i.e. given plan to go from A to S to F to B, which appears it will work
                    [A, S, F, B]
                    but suppose we know at S the road to F sometimes closed (and sign there tells us),
                    but when start off we cannot read that sign
                    i.e. when start, we may not know what state we are in (i.e. may know
                    we are in state A, but may not know if road is open or closed until
                    we get to S in order to know if can continue down that path or not)
                * **Unknown**
                    i.e. due to lack of knowledge on our part, where some model of world is unknown
                    (such as map or GPS that is inaccurate or incomplete), then may not be able to execute
                * **Hierarchical**
                    i.e. may need to deal with plans that are hierarchical at a high level steps
                    that cannot be executed (i.e. action of go from A to S) without
                    detailed low level plan steps (i.e. turn right, left, accelerate more, etc) being scheduled
                    during execution
        * Example 1: Vacuum Cleaner (**Deterministic environment**)
            * Add:
                ```
                Locations
                =========
                A - Left Location
                B - Right Location
                V - Vacuum
                D1, D2 - Dirt (Yes or No)

                Possible States == 2 * 2 * 2 == 8

                State Space diagram - shows 3x possible Actions transitioning between states
                    - Moving Right
                    - Moving Left
                    - Dirt Sucking
                ```

                * Given **"Fully Deterministic" and "Fully Observable"** world it is easy to plan
                    i.e. Start State to Goal State where both sides clean

                    0) Initally in Left Location
                    1) Execute Action to Dirt Suck
                    2) Move to Right Location
                    3) Execute Action to Dirt Suck
                    4) Transition to Goal State of no dirt


                * Given **"Unobservable" (Sensorless) and Deterministic world** where Vacuum's
                sensors stop working, so can't determine current State (i.e.
                what Location it is in or whether any Dirt).

                    * How does the AI-agent represent the State of the world?

                    * Solution: Search in the State Space of **Belief States**
                    (instead of the State Space of Actual Spaces)
                    i.e. we believe we are in one of the 8 states, and
                    when we execute a subsequent Action we **go to another Belief State
                    and gain knowledge about the world even without sensing**,
                    which is either a known, or unknown state

                        * i.e. Move Right, according to State Space of
                        **Belief States** diagram we know we're in Right-Hand Location
                        (either Moved from Start to Right Wall or we were already
                        at the Right Wall and stayed there)
                            * Note: Reduces possibilities from 8 down to 4
                            so know more about world even though haven't observed anything

                * Note: In real-world, operations of Move Left and Move Right are
                **inverses** of each other
                * **Conformant Plans** Plans that reach the goal without ever
                observing the world
                    i.e. to achieve Goal of clean location, simply Dirt Suck,
                    where move from Initial State where 8 possibilities to
                    one of 4 possible states where one or both Locations are clean
                    (do not know which of the 4 states we are in, but we know
                    we achieved the Goal)

                * Given **"Partially Observable" Vacuum Cleaner in Deterministic world**

                    * Given **Local Sensing** - where Vacuum knows
                    its current Location, and sub-states going on i.e. if dirty or clean,
                    but does not know about sub-state of other Locations (if contain dirt or not)

                    * Draw **Partial Diagram** of part of Belief State from that world,
                    where **Belief State** unfolds as the following occurs:
                        * 1) Initial State
                        * 2) Action taken to Move Right
                        * 3) Observation (in **Act-Percept Cycle**) allows
                        us to split the Belief State of our world to say:

                            * if we observe we are in Location B and it's Dirty
                            then we know where in a certain state according to the diagram,
                            and if observe we're in Location B and its Clean we know we're in
                            another state according to the diagram
        * **Act-Percept Cycle**
            * In a Deterministic world, each individual **world state** (common to multiple
            states) in a Belief State maps exactly to another one (reason why we say
            "deterministic", so the **size** of the Belief State either stays the same
            or may decrease if two Actions accidentally take you to the same Location
        * **Observation**
            * Works opposite of **Act-Percept Cycle**, in that when we
            Observe the world, we're taking the current Belief State and
            partitioning it into pieces of where we know they are.
            Observations cannot introduce a new **world state** into the Belief State.
            Cannot learn less about the world after an Observation.
        * **Stochastic** (non-Deterministic)
            * Multiple outcomes for an Action

        * Example 2: Robot with slippery wheels (**Stochastic and Partially Observable Environment**)
            * Given Robot:
                * Which has slippery wheels
                * Where Move Left or Move Right causes wheels to slip,
                so most of time No Location Change, but sometimes Location Changes
                * Assume Suck always works perfectly
            * Draw **Belief State** Diagram where result of Actions often result
            in a **Belief State** increase (larger than before),
            since Action increases **Uncertainty** as do not know
            what result of Action will be
                * For each of individual **world states** belonging to a Belief State
                we have multiple outcomes for the Action (meaning of Stochastic),
                so end up with larger Belief State, since **Actions tend to increase Uncertainty**
                (in Stochastic and Partially Observable environment)
                * Observation-wise, same holds as in Deterministic world,
                where observation partitions Belief State into smaller Belief States
                and **Observations tend to reduce the Uncertainty**
                (in Stochastic and Partially Observable environment)
            * Planning approach
                i.e. want to go from initial Belief State to one where all squares are clean,
                need to know if will Always work or Maybe sometimes work for possible plans?

                All are Maybe, since with a plan that is a linear sequence, we are never
                guaranteed to achieve goal with **Finite Sequence** of Actions, since
                a successful plan must include at least one move action, but if try
                move action a finite number of times, each of those times the wheels may slip
                and won't move.

                Note: We can introduce a new notion of plans that include
                **Infinite Sequences**

        * **Infinite Sequences**
            * Instead of writing plans as Linear Sequence,
            i.e.
                ```
                (suck, move right, suck)

                [SRS]
                ```
            instead write as **Tree Structure**
            * **Tree Structure** (where Finite Representation represents an
            Infinite Sequence of plans)
                * New State - Starting **Belief State**
                * Action - Suck
                * New State - A
                * Action - Move Right
                * Observation - either [A, Clean] or [B, Clean] or [B, Dirty]
                  of current State
                    * If identifies we are still in previous state,
                    then go back in plan to A
                    * Else continue
                * Action - Suck
            * Write infinite sequence of plans in more linear notation:
                ```
                S, while we observe A do R, and then do S

                [S, while A: R,S]
                ```
            * Note: If Stochasticity is independent
            (i.e. sometimes works and sometimes it doesn't work),
            then with Probability of 1 as Limit, then this Plan
            will achieve the Goal. However, can't state any
            bound qty of steps with which can achieve the goal,
            can only say it's infinite

        * **Finding a Successful Plan**

            * **Approach with Problem Solving (using Trees)** (Previously)
                * Start in Single State (not a Belief State)
                * Search possible states in a Tree
                until find one pair that takes us to Goal State
                * Pick single path from Tree (sequence of steps
                with no branches in it where one leaf node is goal)

            * **Approach with Problem Solving (using Maths Notation)**
                * Given plan with straight line sequence
                    ```
                    [A,S,F]
                    ```
                * Decide if is plan that satisfies the goal by
                checking if end state is goal state


                * Mathematical formulation for plan to be a goal
                    ```
                    1 Started in start state S1
                    2 Transitioned to state S2 that is result of
                      applying to that start state S1 the action
                      of going from A to S
                    3 Apply the result of starting
                      in intermediate state S2 and applying action
                      of going from S to F
                    4 Check if resulting tate is an element of
                      the set of goals to see if plan is valid
                      and gives us a solution

                      Result (Result (A, A -> S), S -> F) <- Goals

                    ```
                * **Stochastic and Partially Observable**
                    * Note: In Stochastic and Partially Observable worlds
                    the equations are more complicated

                    * Instead of just dealing with **Individual States**
                    such as getting result of applying action to
                    initial state (i.e. `s1 = Result(s, a)`), we're
                    actually dealing with **Belief States** where
                    we making Prediction where we start
                    in a belief state "b"
                    and we look at the action "a" and each possible
                    result of the action (since they're stochastic
                    to each possible member of the Belief State, which
                    gives us a larger Belief State), then we
                    Update the Belief State by taking the Observation "o"
                    into account (which gives the same or smaller sized
                    Belief State), and the we get a new state "b1"

                    * Note: Use this **Predict-Update Cycle**
                    to Track where we are
                        * tells everything we need to know
                        * Weakness: Belief States start to get large
                        since they just list all the possible world states
                            * Alternative: Instead represent the possible
                            states using Variables (i.e. using
                            Classical Planning) and an overarching
                            Formula over the Variables that describes the
                            states

                    ```
                    b1 = Update(Predict(b, a), o)
                    ```

            * **Approach with Belief States and Branching Plan Structures**
                * Same process as for "Problem Solving",
                but more complex tree with
                different possibilities
                * Example (of searching through tree)
                    * Init State
                        * Possible Actions to for Next State
                            * Move Right
                            * Suck
                        * Branch of the Plan (not part of Search Space)
                    * Try each Action and expanding nodes
                    until find a Portion of the tree that is
                    a Successful Plan according to criteria of reaching
                    a Goal

            * **Unbounded Solutions**
                * If every leaf node in a plan is a goal since we
                can't guide it in one direction or another
                (no matter what path with execute based on observations,
                noting that observations come to us, we do not pick them)

            * **Bounded Solution**
                * No loops (do not know how many steps it will take),
                but ok to have branches

        * **Notation of Classical Planning (i.e. Action Schema)**
            * Dfn:
                * Representation language for dealing with States, Actions,
                and Plans
                * Used as approach to deal with complexity by factoring the
                world into **Variables**
                * State Space
                    * All possible assignments to k-Boolean Variables
                * State Space Size - 2^k
            * Example - Vacuum world with 2 Locations
                * States
                    * Possible States: 2*2*2 = 8
                    (succinctly represented through the 3 Variables below)
                    * Variables: 3 (Boolean)
                        * DirtA - Dirt in Location A
                        * DirtB - in Location B
                        * VacA - Vacuum in Location A
                    * World State - Complete Assignment of True or False
                    to each of the 3 Variables
                    * Belief States (depends on type of env we want to deal with)
                        * Complete Assignment (in Core Classical Planning,
                        useful in Fully Deterministic Fully Observable domains)
                        * Partial Assignment (by extending Classical Planning)
                        i.e. Belief State where VacA is True, whereas other 2 Variables
                        are unknown, such that formula represents 4 possible world states
                        * Arbitrary Formula (in boolean logic)

                * Actions
                    * Represented in Classical Planning by **Action Schema**
                    since represents a set of actions (i.e. for all possible planes,
                    for all x and y)
                    * Example 1: Fly Cargo
                        * Given
                            * Goal - need to send cargo around world
                            * Assets - plans at airports, cargo
                            * Action Schema (of plane flying from one
                            location to another) -
                                ```
                                Action - since Action Schema
                                Operator and Args - i.e. Fly from x to y
                                Preconditions - what need to know to be true
                                  to execute actions
                                    - P must be a plane
                                    - ^ - AND from Boolean Propositional Logic
                                    - At - p better be at x
                                Effects (what will happen as a result of action
                                in terms of transition between state spaces) -
                                  i.e. when fly from x to y, the plane is no longer at x

                                Action (Fly(p, x, y)
                                    PRECOND: Plane (p) ^ Airport (x) ^ Airport (y)
                                                       ^ At (p, x)
                                    EFFECT: ¬ At (p, x) ^ At (p, y)
                                ```
                        * Note: Can apply the Action Schema to specific world states
                        where p and x would have specific values (we concatenate the names
                        together to come up with one variable where the name has a complex form
                        with commas and parenthesis to make easier to write one Action Schema
                        that covers all individual flying actions)

                    * Example 2: Fly Cargo
                        * More complete representation of Problem Solving domain
                        in language of classical planning
                        * Note: Below representation and algorithms is good enough to
                        handle 100,000+ cargo planes, and representing millions of ground actions
                        at dozens of airports.
                        If 100 planes, would be 10,000 different fly actions
                        If had 1000's of cargo pieces we'd have more Load and Unload actions
                        (all representable by the below succinct Schema)
                        *
                        ```
                        ###################
                        # Initial State (2x Cargo pieces, 2x Planes, 2x Airports) and wher things are
                        ###################
                        Init(
                              At(C1, SFO) ^ At(C2, JFK) ^ At(P1, SFO) ^ At(P2, JFK)
                              ^ Cargo(C1) ^ Cargo(C2) ^ Plane(P1) ^ Plane(P2)
                              ^ Airport(JFK) ^ Airport(SFO)
                            )
                        ###################
                        # Goal State (i.e. C1 cargo must be delivered to JFK airport and...)
                        ###################
                        Goal(
                              At(C1, JFK) ^ At(C2, SFO)
                            )
                        ###################
                        # Operator "Load" (loads cargo "c" onto Plane "p" at Airport "a")
                        ###################
                        Action(
                              Load(c,p,a),
                              PRECOND: At(c, a) ^ At(p, a) ^ Cargo(c) ^ Plane(p) ^ Airport(a)
                              EFFECT: ¬ At(c, a) ^ AIn(c, p)
                              )

                        ###################
                        # Operator "Unload" (unloads cargo "c" from Plane "p" at Airport "a")
                        ###################
                        Action(
                              Unload(c, p, a),
                              PRECOND: In(c, p) ^ At(p, a) ^ Cargo(c) ^ Plane(p) ^ Airport(a)
                              EFFECT: At(c,a) ^ ¬ In(c,p)
                              )
                        ###################
                        # Fly Action Schema
                        ###################
                        Action(
                              Fly(p, from, to,
                              PRECOND: At(p, from) ^ Plane(p) ^ Airport(from) ^ Airport(to)
                              EFFECT: ¬ At(p, from) ^ At(p, to)
                              )

                        ```

        * **Progression (Forward) State Space Search - Planning using Notation of Classical Planning (i.e. Action Schema)**
            * Searching through Concrete world states
            * Planning using Action Schema is done same as we did planning for Problem Solving
            * Searching through space of exact states, where each state is an Individual World State
            and where Actions are Deterministic then its the same approach as in Problem Solving
            * Since we are using this representation using Action Schema, there are other possibilities
            available

            * Example: Fly Cargo
                * Initial State `At(P1, SFO), At(C1, SFO)`
                * Branching to possible Actions
                    i.e. one possible Action is to Load `Load(C1, P1, SFO)`
                    * Continue Branching until satisfy Goal predicate

        * **Regression (Backwards) State Space Search**
            * Searching through Abstract states (where some Variables unspecified)
            * Where we start at the Goal representation (of many world states)
            * Use the same representation for arrows that represent a set of possible Actions

            * Example: Fly Cargo
                * Take description of Goal State `At(C1, JFK), At(C2, SFO)`
                (all we know about the state is that these two Propositions are True)
                * Backward Search by asking what Actions would lead to that Goal State,
                which represents a whol family of States with different Variables and values
                * Individually inspect one at a time the "Definition" of each possible Actions
                that would result in this each of the part of the Goal
                    * i.e. inspect Actions in Action Schema that would result in `At(C1, JFK)`,
                    where we find the only Action that results in an `At` is the
                    Unload Action Schema, such that `Unload(c,p,a)` results in `At(c,a)`,
                    so we know to achieve goal we'd have to perform an Action
                    `Unload(C1, anything, JFK)`,
                    (i.e. branch leading into Goal state is `Unload(c,p,a)`, which
                     represents all possible Actions for any Plane "p" for Unloading cargo
                     at the destination)
                * Regress the State over the Operator to get another representation before the Action
                that also will be Unknown (i.e. do not know what "p" is involved)
                * Continue searching backwards until enough of Variables are filled in
                and where we match against Initial State so we have our Solution that we found
                going Backwards
                * Apply the found Solution going Forwards

            * Example: Buying a Book
                ```
                # Single Action (buying a book)
                # Precondition is identifying what book and Effect is that own the book
                # Goal is to own the book satisfying precondition ISBN
                Action(buy(b)
                    PRECOND: ISBN(b)
                    EFFECT: OWN(b)
                Goal (Own(0136042597))
                ```
                * **Progression** Forward Search Solution:
                    * Initial State - own nothing
                    * Actions - which possible to apply
                        i.e. if 10,000,000 possible books
                             then branching factor is 10,000,000
                             coming out of initial state node
                             and have to try all in order until hit
                             goal
                        **Very Inefficient**
                * **Regression** Backward Search Solution:
                    * Goal State - own ISBN No. 0136042597
                    * Actions - Inspect possible ones
                                out of the 10,000,000,
                                where there is Action Schema,
                                which can only match Goal in one way
                                when "b" == 0136042597
                                so we know Action is to
                                buy 0136042597, so able to connect
                                Goal State to Initial State in
                                backwards direction in one step

        **Plan Space Search**
            * Dfn: Searching through space of plans
            (rather than searching through space of states)
            * Note:
                * Plan Space Search - Old approach
                * Forward Search - Popular as easier to come up with good Heuristics for search,
                                   as deals with Concrete Plan States

                    * Example: Sliding Puzzle (using **Heuristics**)
                        * **Action Schema** for puzzle is formal representation
                        of **encoding Actions in logical form, so a program
                        could modify and generate Heuristics from it
                        (rather than requiring a human to manually do it)**

                            ```
                            # Action - slide tile to from location a to b
                            Action(Slide (t,a,b)

                                    # Precondition - tile must be on location a,
                                      and must be a tile, and location b
                                      must be blank, and location a and b
                                      must be adjacent

                                    PRECOND: On(t,a) ^ Tile(t) ^ Blank(b) ^ Adj(a,b)

                                    # Effect - tile is now on location b, and blank
                                      is now on location a, and tile is no longer on
                                      location a, and blank is no longer on location b

                                    EFFECT: On(t,b) ^ Blank(a) ^
                                            ¬ On(t,a) ^ ¬ Blank(b)
                                  )

                            ```
                        * Use **Action Schema** to automatically derive
                        good **Heuristics** by modifying original Problem Heuristic
                            * Heuristic #1
                                * Remove a Precondition to make the problem easier
                                and generate a different Heuristic.
                                i.e. remove `Blank(b)` to end up with
                                the **"Manhattan" (aka City Block) Heuristic**
                                (i.e. since not Blank, and not contain Tile, it
                                contains something else)
                            * Heuristic #2
                                * i.e. remove `Adj(a,b)` to get the
                                **Qty Misplaced Tiles Heuristic**
                                (i.e. allows sliding a tile from any location a
                                to any location b, no matter how far apart they)
                            * Heuristic #3 (Ignore negative effects)
                                i.e. remove `¬ Blank(b)` from EFFECT,
                                to make the problem easier (more relaxed problem)

                * Backward - Benefit as may be more Efficient than Forward Search

            * Example: Getting Dressed with clothes in right order
                * Initial Empty Plan (flawed as does not lead from Start to Goal)
                    ```
                    Start State
                    Goal State
                    ```
                * Update Plan
                    * Add "Right Shoe" Operator
                        ```
                        Start State
                        Right Shoe
                        Goal State
                        ```
                * Check if updated plan solves problem, otherwise further refine plan
                * Update Plan
                    * Add parallel sequence of Operators in branching structure
                        * "Left Shoe"
                        * "Right Shoe"
                    * Note: Captures that can achieve in any order
                    ```
                        Start State
                         /       \
                    Left Shoe    Right Shoe
                         \       /
                        Goal State
                    ```
                * Repeat until get plan that works

        **Planning Representation**

            * **Situation Calculus (using FOL for Planning)**
                * Dfn: Regular FOL with a set of conventions for how to
                represent states and actions
                * **Possibility Axioms** and **Successor-State Axioms** are most of what
                comprises Situational Calculus and are used to describe entire
                **Domains** (i.e. airport cargo domain).
                We describe our specific Problem within that Domain by
                describing the Initial State/Situation, where we make different
                types of assertions using different predicates
                * Note: Deriving Solution from a Problem described in
                that supports full power of FOL using Situation Calculus
                where we can write anything we want
                (much more flexible than in Problem Solving or Classical Planning)
                and doesn't need
                any special program since there are already Theorem Provers for FOL,
                so simply state as a Problem, apply the Normal Theorem Prover
                that we already had for other uses so
                it comes up with a Situation as an answer which
                corresponds to a Path that satisfies the Goal, given the Initial
                State and Action descriptions.

                * **Conventions**
                    * **Actions** represented as Objects in FOL (normally by functions)
                        ```


                        # Function Fly - represents an Object that is the Action
                        # (i.e. fly plane from airport x to y)

                        Fly(p,x,y)
                        ```
                    * **Situations** described by Objects in the logic and
                    correspond to the **Past History of Actions** that
                    are in state space search

                        i.e. arriving at same world state from two different sets of
                        actions, they would be considered two different situations

                        ```
                        # Initial Situation
                        S0

                        # Function of Situations (aka "Result") where
                        result of situation object "s" and action object
                        "a" equals another situation S'

                        S' = Result(s, a)

                        # Possible

                            # Note: Situation Calculus does not describe Actions
                            applicable in a Situation with predicate Actions of
                            "s" as `Actions(s)`. Instead Situation Calculus talks
                            about Actions that are possible in a state by using
                            a predicate `Poss(a,s)`, which is an Action "a" that is
                            possible in a state

                        Poss(a, s)

                        # Precondition

                            # Note: Form for describing predicates (i.e. Poss(a,s)),
                            which has the form of a Precondition of state "s"
                            that implies it's possible to do Action "a" in
                            state "s"

                        SomePrecond(s) -> Poss

                        # Possibility Axiom for the Action "Fly"

                            # if some "p" exists which is a Plane in state "s" and
                            there's some "x" which is an Airport in state "x" and
                            there's also some "y" which is also an Airport in state "s" and
                            "p" is at Location "x" in state "s", then that implies
                            it's possible to fly "p" from "x" to "y" in state "s"

                        Plane(p,s) ^ Airport(x,s) ^ Airport(y,s) ^ At(p,x,s) -> Poss(Fly(p,x,y))

                        ```

                    * **Predicates**

                        * **Fluents** (fluidity or change over time)
                        ```
                        i.e.

                        # Plane "p" at Airport "x" in Situation "s"

                        At(p,x,s)
                        ```

                    * **Possibility Axiom** `Poss(...)`

                    * **Successor-State Axiom** `In(...)`

                    * **Describing what Changed and what Does Not Change
                    in Classical Planning vs Situational Calculus**
                        * **Classical Planning** had "Action Schemas" that
                        described one Action at a time and what changed
                        * **Situation Calculus** however does it the other way around
                        by instead of writing one Action, Schema, or Axiom for
                        each Action, we instead do one for each Fluent (Predicate)
                        that can change using a **convention** called
                        **Successor-State Axioms (SSA)** that describe what happens in the
                        state that is a Successor of executing an Action.
                        SSAs have the form of saying:
                        ```
                        i.e. For All Actions and States, if it's possible to execute
                        Action "a" in State "s", then the Fluent is True
                        if and only if (IFF) Action "a" made it True OR
                        if Action "a" didn't undo it

                        (i.e. either it wasn't true before and "a" made it True,
                        or it was true before and "a" didn't stop it from being True)

                        ∀a,s Poss(a,s) -> (fluent true <-> "a" made it true V "a" didn't undo it)
                        ```
                        * Example: **Successor State Axiom** for the `In` Predicate

                        ```
                        i.e. For All Actions and States, for which it's Possible to execute
                        Action "a" in Situation "s", and if that's true then the In
                        Predicate holds between some cargo "c" in some plane "p" which is
                        in the state that is the "result" of executing Action "a" in state "s",
                        so the In Predicate will hold, IFF "a" was a Load Action
                        (i.e. if we load the cargo into the plane, then the result of
                        executing that Action "a" is that the cargo is in the Plane "p")
                        OR it might be already true that the cargo was in the Plane "p" in
                        Situation "s", AND Action "a" is not an Unload Action

                        ∀(a,s) Poss(a,s) -> In(c,p,result(s,a)) <-> (a=Load(c,p,x) V (In(c,p,s) ^ "a" != Unload(c,p,x)))

                        # Predicate states valid sentences in FOL like the following
                        to asset the initial state
                            - Plane P1 is at Airport JFK in Situation S0
                            - For All C, if C is Cargo then that C is at JFK
                            in Situation S0

                        Initial state: S0
                            At (P1,JFK,S0)
                            ∀c Cargo(c) -> At(c,JFK,S0)

                        # There Exists a Goal State "s" such that For All "c"
                        if "c" is Cargo, we want all that cargo
                        to be as SFO in state "s'

                        Goal:
                            Ǝs ∀c Cargo(c) -> At(c,SFO,s)

                        ```

                * Example: Fly Cargo
                    * Goal: Move all cargo from Airport A to Airpot B,
                            regardless of qty of cargo
                        * Note: In FOL, you can express the notion of "All",
                        but NOT in Propositional Languages like
                        Classical Planning Notation

        * **Predicates** i.e. `At(...)`

        * **Complete Assignment** Dfn: when all Variables have values
        * **Partial Assignment** Dfn: when some Variables have values and others do not









